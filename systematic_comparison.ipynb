{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACP Project - Systematic Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, pickle, os, itertools\n",
    "from dataclasses import dataclass\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "try:\n",
    "    from sklearnex import patch_sklearn\n",
    "    patch_sklearn()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 300)\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "\n",
    "import shap\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Notebook:\n",
    "    CROSS_VAL_N_JOBS = 1\n",
    "    N_PROCESSES = 1\n",
    "    N_TRIALS = 1\n",
    "    TRIALS_TIMEOUT = 60 * 60 * 2\n",
    "    TRIAL_STORAGE = None  # \"sqlite:///models/studies.db\"\n",
    "    RUN = False\n",
    "    PERSIST_MODELS = True\n",
    "    MODEL_DIR = \"models/systematic\"\n",
    "\n",
    "\n",
    "if Notebook.PERSIST_MODELS:\n",
    "    try:\n",
    "        os.makedirs(Notebook.MODEL_DIR)\n",
    "    except FileExistsError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from dataset import SCIData, SCICols\n",
    "%aimport dataset\n",
    "\n",
    "sci = SCIData.load('data/sci.h5')\n",
    "\n",
    "scii = (\n",
    "    SCIData(SCIData.quickload(\"data/sci_processed.h5\").sort_values(\"AdmissionDateTime\"))\n",
    "    .mandate(SCICols.news_data_raw)\n",
    "    .derive_critical_event(within=1, return_subcols=True)\n",
    "    .augment_shmi(onehot=True)\n",
    "    .omit_redundant()\n",
    "    .raw_news()\n",
    "    .derive_ae_diagnosis_stems(onehot=False)\n",
    "    .categorize()\n",
    "   # .onehot_encode_categories()\n",
    ")\n",
    "\n",
    "sci_train, sci_test, _, y_test_mortality, _, y_test_criticalcare = train_test_split(\n",
    "    scii,\n",
    "    scii.DiedWithinThreshold,\n",
    "    scii.CriticalCare,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    "    shuffle=False,\n",
    ")\n",
    "sci_train, sci_test = SCIData(sci_train), SCIData(sci_test)\n",
    "# (X_train, y_train), (X_test, y_test) = (\n",
    "#     sci_train.xy(outcome=\"CriticalEvent\", dropna=False, fillna=False),\n",
    "#     sci_test.xy(outcome=\"CriticalEvent\", dropna=False, fillna=False),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from typing import Dict, Any, Iterable\n",
    "\n",
    "\n",
    "class Estimator:\n",
    "    _name: str\n",
    "    _estimator: BaseEstimator\n",
    "    _requirements: Dict[str, bool]\n",
    "    _static_params: Dict[str, Any] = {}\n",
    "    _tuning_params_default: Dict[str, Any] = {}\n",
    "    _fit_params: Dict[str, Any] = {}\n",
    "\n",
    "    def __init__(self, sci_train):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        return dict()\n",
    "\n",
    "    @classmethod\n",
    "    def compile_parameters(cls, params):\n",
    "        return {\n",
    "            f\"{cls._name}__{key}\": value\n",
    "            for key, value in {\n",
    "                **cls._static_params,\n",
    "                **cls._tuning_params_default,\n",
    "                **params,\n",
    "            }.items()\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def factory(cls):\n",
    "        return cls._estimator(**cls._static_params)\n",
    "\n",
    "    @classmethod\n",
    "    def fit_params(cls, X_train, y_train):\n",
    "        return {\n",
    "            f'{cls._name}__{key}': value\n",
    "            for key, value in cls._fit_params.items()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn import FunctionSampler\n",
    "\n",
    "\n",
    "class Resampler_SMOTE(Estimator):\n",
    "    _name = \"SMOTE\"\n",
    "    _estimator = SMOTENC\n",
    "\n",
    "    _static_params = dict(random_state=42, n_jobs=None,)\n",
    "\n",
    "    _tuning_params_default = dict(sampling_strategy=0.1, k_neighbors=5)\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            sampling_strategy=trial.suggest_float(\n",
    "                f\"{cls._name}__sampling_strategy\", 0.1, 0.5\n",
    "            ),\n",
    "            k_neighbors=trial.suggest_int(f\"{cls._name}__k_neighbors\", 2, 10),\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            f\"{cls._name}__kw_args\": {\n",
    "                **cls._static_params,\n",
    "                **cls._tuning_params_default,\n",
    "                **suggestions,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def factory(cls):\n",
    "        return FunctionSampler(\n",
    "            func=SCIData.SMOTE, validate=False, kw_args=cls._static_params\n",
    "        )\n",
    "\n",
    "\n",
    "class Resampler_RandomUnderSampler(Estimator):\n",
    "    _name = \"RandomUnderSampler\"\n",
    "    _estimator = RandomUnderSampler\n",
    "\n",
    "    _static_params = dict(random_state=42, replacement=False)\n",
    "\n",
    "    _tuning_params_default = dict(sampling_strategy=0.1)\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        return cls.compile_parameters(\n",
    "            dict(\n",
    "                sampling_strategy=trial.suggest_float(\n",
    "                    f\"{cls._name}__sampling_strategy\", 0.05, 0.5\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class No_Resampling(Estimator):\n",
    "    _name = \"No_Resampling\"\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        return dict()\n",
    "\n",
    "    @staticmethod\n",
    "    def _(X, y):\n",
    "        return X, y\n",
    "\n",
    "    @classmethod\n",
    "    def factory(cls):\n",
    "        return FunctionSampler(func=cls._, validate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "class Estimator_LightGBM(Estimator):\n",
    "    _name = \"LightGBM\"\n",
    "    _estimator = LGBMClassifier\n",
    "\n",
    "    _requirements = dict(onehot=False, ordinal=False, imputation=False, fillna=False, resampling=False)\n",
    "\n",
    "    _static_params = dict(\n",
    "        objective=\"binary\",\n",
    "        metric=[\"l2\", \"auc\"],\n",
    "        boosting_type=\"gbdt\",\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "    )\n",
    "\n",
    "    _tuning_params_default = dict(\n",
    "        is_unbalance=True,\n",
    "        reg_alpha=1.8e-3,\n",
    "        reg_lambda=6e-4,\n",
    "        num_leaves=14,\n",
    "        colsample_bytree=0.4,\n",
    "        subsample=0.97,\n",
    "        subsample_freq=1,\n",
    "        min_child_samples=6,\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            reg_alpha=trial.suggest_float(\n",
    "                f\"{cls._name}__reg_alpha\", 1e-4, 10.0, log=True\n",
    "            ),\n",
    "            reg_lambda=trial.suggest_float(\n",
    "                f\"{cls._name}__reg_lambda\", 1e-4, 10.0, log=True\n",
    "            ),\n",
    "            num_leaves=trial.suggest_int(f\"{cls._name}__num_leaves\", 2, 256),\n",
    "            colsample_bytree=trial.suggest_float(\n",
    "                f\"{cls._name}__colsample_bytree\", 0.4, 1.0\n",
    "            ),\n",
    "            subsample=trial.suggest_float(f\"{cls._name}__subsample\", 0.4, 1.0),\n",
    "            subsample_freq=trial.suggest_int(f\"{cls._name}__subsample_freq\", 1, 7),\n",
    "            min_child_samples=trial.suggest_int(\n",
    "                f\"{cls._name}__min_child_samples\", 5, 150\n",
    "            ),\n",
    "            is_unbalance=trial.suggest_categorical(\n",
    "                f\"{cls._name}__is_unbalance\", [True, False]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if not suggestions[\"is_unbalance\"]:\n",
    "            suggestions[\"scale_pos_weight\"] = trial.suggest_int(\n",
    "                f\"{cls._name}__scale_pos_weight\", 1, 100\n",
    "            )\n",
    "\n",
    "        r = cls.compile_parameters(suggestions)\n",
    "        if not suggestions[\"is_unbalance\"]:\n",
    "            del r[f\"{cls._name}__is_unbalance\"]\n",
    "\n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "class Estimator_XGBoost(Estimator):\n",
    "    _name = \"XGBoost\"\n",
    "    _estimator = XGBClassifier\n",
    "\n",
    "    _requirements = dict(onehot=False, ordinal=False, imputation=False, fillna=False, resampling=False)\n",
    "\n",
    "    _static_params = dict(\n",
    "        verbosity=0,\n",
    "        n_jobs=1,\n",
    "        objective=\"binary:logistic\",\n",
    "        booster=\"gbtree\",\n",
    "        enable_categorical=True,\n",
    "    )\n",
    "\n",
    "    _tuning_params_default = {\n",
    "        **dict(\n",
    "            tree_method=\"hist\",\n",
    "            alpha=7e-05,\n",
    "            subsample=0.42,\n",
    "            colsample_bytree=0.87,\n",
    "            scale_pos_weight=14,\n",
    "            max_depth=7,\n",
    "            min_child_weight=10,\n",
    "            eta=0.035,\n",
    "            gamma=4e-08,\n",
    "            grow_policy=\"lossguide\",\n",
    "        ),\n",
    "        \"lambda\": 7e-2,\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            tree_method=trial.suggest_categorical(\n",
    "                f\"{cls._name}__tree_method\", [\"approx\", \"hist\"]\n",
    "            ),\n",
    "            alpha=trial.suggest_float(f\"{cls._name}__alpha\", 1e-8, 1.0, log=True),\n",
    "            subsample=trial.suggest_float(f\"{cls._name}__subsample\", 0.2, 1.0),\n",
    "            colsample_bytree=trial.suggest_float(\n",
    "                f\"{cls._name}__colsample_bytree\", 0.2, 1.0\n",
    "            ),\n",
    "            scale_pos_weight=trial.suggest_int(\n",
    "                f\"{cls._name}__scale_pos_weight\", 1, 100\n",
    "            ),\n",
    "            max_depth=trial.suggest_int(f\"{cls._name}__max_depth\", 3, 9, step=2),\n",
    "            min_child_weight=trial.suggest_int(f\"{cls._name}__min_child_weight\", 2, 10),\n",
    "            eta=trial.suggest_float(f\"{cls._name}__eta\", 1e-8, 1.0, log=True),\n",
    "            gamma=trial.suggest_float(f\"{cls._name}__gamma\", 1e-8, 1.0, log=True),\n",
    "            grow_policy=trial.suggest_categorical(\n",
    "                f\"{cls._name}__grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "            ),\n",
    "        )\n",
    "        suggestions[\"lambda\"] = trial.suggest_float(\n",
    "            f\"{cls._name}__lambda\", 1e-8, 1.0, log=True\n",
    "        )\n",
    "\n",
    "        return cls.compile_parameters(suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class Estimator_LogisticRegression(Estimator):\n",
    "    _name = \"LogisticRegression\"\n",
    "    _estimator = LogisticRegression\n",
    "\n",
    "    _requirements = dict(onehot=True, ordinal=False, imputation=True, fillna=True, resampling=False)\n",
    "\n",
    "    _static_params = dict(max_iter=100, solver=\"lbfgs\", random_state=42, penalty=\"l2\")\n",
    "\n",
    "    _tuning_params_default = dict(penalty=\"l2\", C=5.9, class_weight=\"balanced\")\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            penalty=trial.suggest_categorical(f\"{cls._name}__penalty\", [\"l2\", \"none\"]),\n",
    "            C=trial.suggest_float(f\"{cls._name}__C\", 0.01, 10),\n",
    "            class_weight=trial.suggest_categorical(\n",
    "                f\"{cls._name}__class_weight\", [None, \"balanced\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # if suggestions[\"penalty\"] == \"elasticnet\":\n",
    "        #     suggestions[\"l1_ratio\"] = trial.suggest_float(\n",
    "        #         f\"{cls._name}__l1_ratio\", 0.05, 0.95\n",
    "        #     )\n",
    "\n",
    "        return cls.compile_parameters(suggestions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "class Estimator_RandomForest(Estimator):\n",
    "    _estimator = RandomForestClassifier\n",
    "    _name = \"RandomForest\"\n",
    "\n",
    "    _requirements = dict(onehot=False, ordinal=True, imputation=False, fillna=True, resampling=False)\n",
    "    _tuning_params_default = dict(\n",
    "        n_estimators = 250,\n",
    "        max_features=0.56,\n",
    "        min_samples_split=8,\n",
    "        min_samples_leaf=3,\n",
    "        max_samples=0.75,\n",
    "        class_weight='balanced',\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            n_estimators = trial.suggest_int(\n",
    "                f'{cls._name}__n_estimators', 25, 250\n",
    "            ),\n",
    "            max_features = trial.suggest_float(\n",
    "                f'{cls._name}__max_features', 0.15, 1.0\n",
    "            ),\n",
    "            min_samples_split = trial.suggest_int(\n",
    "                f'{cls._name}__min_samples_split', 2, 15\n",
    "            ),\n",
    "            min_samples_leaf = trial.suggest_int(\n",
    "                f'{cls._name}__min_samples_leaf', 1, 15\n",
    "            ),\n",
    "            max_samples = trial.suggest_float(\n",
    "                f'{cls._name}__max_samples', 0.5, 0.99\n",
    "            ),\n",
    "            class_weight = trial.suggest_categorical(\n",
    "                f'{cls._name}__class_weight', [None, 'balanced', 'balanced_subsample']\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return cls.compile_parameters(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.isolation_forest_wrapper import IsolationForestWrapper\n",
    "class Estimator_IsolationForest(Estimator):\n",
    "    _name = 'IsolationForest'\n",
    "    _estimator = IsolationForestWrapper\n",
    "    _requirements = dict(onehot=True, ordinal=False, imputation=True, fillna=True, resampling=False)\n",
    "\n",
    "    _tuning_params_default = dict(\n",
    "        n_estimators = 140,\n",
    "        max_samples = 0.45,\n",
    "        contamination = 0.02,\n",
    "        max_features = 0.69,\n",
    "        bootstrap=False\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            n_estimators = trial.suggest_int(\n",
    "                f'{cls._name}__n_estimators', 1, 200\n",
    "            ),\n",
    "            max_samples = trial.suggest_float(\n",
    "                f'{cls._name}__max_samples', 0.0, 1.0\n",
    "            ),\n",
    "            contamination = trial.suggest_float(\n",
    "                f'{cls._name}__contamination', 1e-6, 1e-1\n",
    "            ),\n",
    "            max_features = trial.suggest_float(\n",
    "                f'{cls._name}__max_features', 0.0, 1.0\n",
    "            ),\n",
    "            bootstrap = trial.suggest_categorical(\n",
    "                f'{cls._name}__bootstrap', [True, False]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return cls.compile_parameters(suggestions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "@dataclass\n",
    "class TabNetWrapper(TabNetClassifier):\n",
    "    weights: int = 0\n",
    "    max_epochs: int = 100\n",
    "    patience: int = 10\n",
    "    batch_size: int = 1024\n",
    "    virtual_batch_size: int = 128\n",
    "    drop_last: bool = True\n",
    "    eval_metric: str = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return super().fit(X_train=X.to_numpy(), y_train=y.to_numpy(), eval_metric = self.eval_metric, weights=self.weights, max_epochs=self.max_epochs, patience=self.patience, batch_size=self.batch_size, virtual_batch_size=self.virtual_batch_size, drop_last=self.drop_last)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return super().predict(X.to_numpy())\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return super().predict_proba(X.to_numpy())\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return self.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator_TabNet(Estimator):\n",
    "    _estimator = TabNetWrapper\n",
    "    _name = 'TabNet'\n",
    "    _requirements = dict(onehot=False, ordinal=True, imputation=True, fillna=True, resampling=False)\n",
    "\n",
    "    _static_params = dict(\n",
    "        optimizer_fn = torch.optim.Adam,\n",
    "        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        verbose = 0,\n",
    "        device_name=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        scheduler_params = dict(\n",
    "            mode = 'min',\n",
    "            min_lr = 1e-5,\n",
    "            factor = 0.5\n",
    "        ),\n",
    "        optimizer_params = dict(\n",
    "            lr = 2e-2,\n",
    "            weight_decay=1e-5\n",
    "        ),\n",
    "        cat_emb_dim = 1,\n",
    "        max_epochs = 50,\n",
    "        eval_metric='average_precision',\n",
    "        weights=1,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    _tuning_params_default = dict(\n",
    "        n_d = 8,\n",
    "        n_a = 8,\n",
    "        n_steps = 3,\n",
    "        gamma = 1.2,\n",
    "        lambda_sparse = 8e-4,\n",
    "        mask_type='sparsemax',\n",
    "        n_shared=3,\n",
    "        scheduler_params=dict(\n",
    "            patience=5\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def __init__(self, sci_train):\n",
    "        self._categorical_idxs, self._categorical_dims = sci_train.describe_categories(dimensions=True)\n",
    "    \n",
    "    def factory(self):\n",
    "        return self._estimator(\n",
    "            cat_idxs = self._categorical_idxs,\n",
    "            cat_dims = [_+1 for _ in self._categorical_dims], # Because we may add 1 category when we fill_na\n",
    "            **self._static_params\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            n_steps = trial.suggest_int(\n",
    "                f'{cls._name}__n_steps', 1, 10\n",
    "            ),\n",
    "            n_shared = trial.suggest_int(\n",
    "                f'{cls._name}__n_shared', 1, 10\n",
    "            ),\n",
    "            gamma = trial.suggest_float(\n",
    "                f'{cls._name}__gamma', 1, 1.5\n",
    "            ),\n",
    "            lambda_sparse = trial.suggest_float(\n",
    "                f'{cls._name}__lambda_sparse', 1e-6, 1e-3, log=True\n",
    "            ),\n",
    "            mask_type = trial.suggest_categorical(\n",
    "                f'{cls._name}__mask_type', ['entmax', 'sparsemax']\n",
    "            ),\n",
    "            scheduler_params = dict(\n",
    "                patience = trial.suggest_int(\n",
    "                    f'{cls._name}__scheduler__patience', 3, 10\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        n_da = trial.suggest_int(\n",
    "            f'{cls._name}__n_da', 4, 32, \n",
    "        )\n",
    "        suggestions['n_d'], suggestions['n_a'] = n_da, n_da\n",
    "\n",
    "        return cls.compile_parameters(suggestions)\n",
    "\n",
    "    @classmethod\n",
    "    def compile_parameters(cls, params):\n",
    "        r = {\n",
    "            **cls._static_params,\n",
    "            **cls._tuning_params_default,\n",
    "            **params,\n",
    "            'scheduler_params': {\n",
    "                **cls._static_params['scheduler_params'],\n",
    "                **params['scheduler_params']\n",
    "            }\n",
    "        }\n",
    "        return {\n",
    "            f\"{cls._name}__{key}\": value\n",
    "            for key, value in r.items()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_studies(sci_train):\n",
    "    news = SCICols.news_data_raw\n",
    "    news_extended = SCICols.news_data_raw + SCICols.news_data_extras\n",
    "    labs = SCICols.blood\n",
    "    hospital = [\n",
    "        \"AdmissionMethodDescription\",\n",
    "        \"AdmissionSpecialty\",\n",
    "        \"SentToSDEC\",\n",
    "        \"Readmission\",\n",
    "    ]\n",
    "    ae = [\"AandEPresentingComplaint\", \"AandEMainDiagnosis\"]\n",
    "    diagnoses = [_ for _ in sci_train.columns if _.startswith(\"SHMI__\")]\n",
    "    phenotype = [\"Female\", \"Age\"]\n",
    "\n",
    "    return list(\n",
    "        dict(\n",
    "            news=news,\n",
    "            news_extended=news_extended,\n",
    "            news_with_phenotype=news_extended + phenotype,\n",
    "            with_ae_notes=news_extended + phenotype + ae,\n",
    "            with_labs=news_extended + phenotype + labs,\n",
    "            with_notes_and_labs=news_extended + phenotype + ae + labs,\n",
    "            with_hospital=news_extended + phenotype + hospital,\n",
    "            with_notes_and_hospital=news_extended + phenotype + ae + hospital,\n",
    "            with_labs_and_hospital=news_extended + phenotype + labs + hospital,\n",
    "            with_labs_and_diagnoses=news_extended + phenotype + labs + diagnoses,\n",
    "            all=news_extended + phenotype + ae + labs + hospital + diagnoses,\n",
    "        ).items()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_studies(sci_train, study_grid = None):\n",
    "    if study_grid is None:\n",
    "        study_grid = dict(\n",
    "            estimator=[Estimator_LogisticRegression, Estimator_LightGBM, Estimator_XGBoost],\n",
    "            resampler=[None, Resampler_RandomUnderSampler, Resampler_SMOTE],\n",
    "            features=get_feature_studies(sci_train),\n",
    "        )\n",
    "\n",
    "    k, v = zip(*study_grid.items())\n",
    "    return [dict(zip(k, _)) for _ in itertools.product(*v)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class Pipeline(ImbPipeline):\n",
    "    def persist(self, filename):\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(self, file)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, \"rb\") as file:\n",
    "            return pickle.load(file)\n",
    "\n",
    "\n",
    "class PipelineFactory:\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator: Estimator,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        resampler: Optional[Estimator] = None,\n",
    "    ):\n",
    "        (self._estimator, self._resampler, self._X_train, self._y_train,) = (\n",
    "            estimator,\n",
    "            resampler,\n",
    "            X_train,\n",
    "            y_train,\n",
    "        )\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        steps = [\n",
    "            (self._estimator._name, self._estimator.factory(),),\n",
    "        ]\n",
    "        if self._resampler is not None:\n",
    "            steps = [\n",
    "                (\n",
    "                    self._resampler._name,\n",
    "                    self._resampler.factory(),\n",
    "                ),\n",
    "            ] + steps\n",
    "\n",
    "        return Pipeline(steps=steps).set_params(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "class Objective:\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator: Estimator,\n",
    "        resampler: Estimator,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=\"average_precision\",\n",
    "        persist_model=True,\n",
    "    ):\n",
    "        (\n",
    "            self._estimator,\n",
    "            self._resampler,\n",
    "            self._X_train,\n",
    "            self._y_train,\n",
    "            self._cv,\n",
    "            self._scoring,\n",
    "            self._persist_model,\n",
    "        ) = (estimator, resampler, X_train, y_train, cv, scoring, persist_model)\n",
    "\n",
    "        self._best_score = 0\n",
    "        self._best_model = None\n",
    "\n",
    "        self._pipeline_factory = PipelineFactory(\n",
    "            estimator=self._estimator,\n",
    "            resampler=self._resampler,\n",
    "            X_train=self._X_train,\n",
    "            y_train=self._y_train,\n",
    "        )\n",
    "\n",
    "        self._fit_params = self._estimator.fit_params(self._X_train, self._y_train)\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        trial_params = {\n",
    "            **(self._resampler.suggest_parameters(trial) if self._resampler else {}),\n",
    "            **self._estimator.suggest_parameters(trial),\n",
    "        }\n",
    "        model = self._pipeline_factory(**trial_params)\n",
    "\n",
    "        score = cross_validate(\n",
    "            model,\n",
    "            self._X_train,\n",
    "            self._y_train,\n",
    "            cv=self._cv,\n",
    "            scoring=self._scoring,\n",
    "            n_jobs=Notebook.CROSS_VAL_N_JOBS,\n",
    "            fit_params=self._fit_params\n",
    "        )[\"test_score\"].mean()\n",
    "\n",
    "        if self._persist_model and (score > self._best_score):\n",
    "            self._best_score = score\n",
    "            self._best_model = self._pipeline_factory(**trial_params).fit(\n",
    "                self._X_train, self._y_train\n",
    "            )\n",
    "\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Iterable, Optional, Tuple\n",
    "\n",
    "\n",
    "def construct_study(\n",
    "    estimator: Estimator,\n",
    "    sci_train: SCIData,\n",
    "    sci_test: SCIData,\n",
    "    features: Tuple[str, Iterable[str]] = (\"All\", []),\n",
    "    resampler: Estimator = None,\n",
    "    cv=5,\n",
    "    scoring=\"average_precision\",\n",
    "    storage=Notebook.TRIAL_STORAGE,\n",
    "):\n",
    "    X_train, y_train = sci_train.xy(\n",
    "        x=features[1],\n",
    "        imputation=estimator._requirements[\"imputation\"],\n",
    "        onehot_encoding=estimator._requirements[\"onehot\"],\n",
    "        ordinal_encoding=estimator._requirements['ordinal'],\n",
    "        fillna=estimator._requirements[\"fillna\"],\n",
    "    )\n",
    "\n",
    "    name = f\"{estimator._name}_{resampler._name if resampler else 'None'}_{features[0]}\"\n",
    "    objective = Objective(\n",
    "        estimator=estimator(SCIData(sci_train[features[1]])),\n",
    "        resampler=resampler(SCIData(sci_train[features[1]])),\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        persist_model=Notebook.PERSIST_MODELS,\n",
    "    )\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=name, storage=storage)\n",
    "\n",
    "    def call(**kwargs):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            study.optimize(objective, **kwargs)\n",
    "\n",
    "        if Notebook.PERSIST_MODELS:\n",
    "            objective._best_model.persist(f\"{Notebook.MODEL_DIR}/{name}\")\n",
    "\n",
    "        return objective._best_model\n",
    "\n",
    "    return call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "No early stopping will be performed, last training weights will be used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2022-11-01 21:40:36,007]\u001b[0m Trial 0 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\stybl\\AppData\\Local\\Temp\\ipykernel_2412\\2572392098.py\", line 43, in __call__\n",
      "    score = cross_validate(\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 266, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\imblearn\\pipeline.py\", line 281, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params)\n",
      "  File \"C:\\Users\\stybl\\AppData\\Local\\Temp\\ipykernel_2412\\162009662.py\", line 15, in fit\n",
      "    super().fit(X_train=X.to_numpy(), y_train=y.to_numpy(), eval_metric = self.eval_metric, weights=self.weights, max_epochs=self.max_epochs, patience=self.patience, batch_size=self.batch_size, virtual_batch_size=self.virtual_batch_size, drop_last=self.drop_last)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\", line 223, in fit\n",
      "    self._train_epoch(train_dataloader)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\", line 434, in _train_epoch\n",
      "    batch_logs = self._train_batch(X, y)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\", line 469, in _train_batch\n",
      "    output, M_loss = self.network(X)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\", line 583, in forward\n",
      "    return self.tabnet(x)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\", line 468, in forward\n",
      "    steps_output, M_loss = self.encoder(x)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\", line 160, in forward\n",
      "    M = self.att_transformers[step](prior, att)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\", line 635, in forward\n",
      "    x = self.bn(x)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\", line 36, in forward\n",
      "    res = [self.bn(x_) for x_ in chunks]\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\", line 36, in <listcomp>\n",
      "    res = [self.bn(x_) for x_ in chunks]\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\", line 168, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"c:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\functional.py\", line 2282, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\stybl\\src\\acute-care-pathways\\systematic_comparison.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m get_studies(sci_train, study_grid)[\u001b[39m5\u001b[39m:]:\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=9'>10</a>\u001b[0m     s \u001b[39m=\u001b[39m construct_study(\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=10'>11</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_, sci_train\u001b[39m=\u001b[39mSCIData(sci_train\u001b[39m.\u001b[39mhead(\u001b[39m1000\u001b[39m)), sci_test\u001b[39m=\u001b[39msci_test\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=11'>12</a>\u001b[0m     )\n\u001b[1;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=13'>14</a>\u001b[0m     r \u001b[39m=\u001b[39m s(n_trials\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\stybl\\src\\acute-care-pathways\\systematic_comparison.ipynb Cell 18\u001b[0m in \u001b[0;36mconstruct_study.<locals>.call\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=34'>35</a>\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=35'>36</a>\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=36'>37</a>\u001b[0m     study\u001b[39m.\u001b[39moptimize(objective, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m Notebook\u001b[39m.\u001b[39mPERSIST_MODELS:\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=39'>40</a>\u001b[0m     objective\u001b[39m.\u001b[39m_best_model\u001b[39m.\u001b[39mpersist(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mNotebook\u001b[39m.\u001b[39mMODEL_DIR\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\optuna\\study\\study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[0;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     _optimize(\n\u001b[0;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\optuna\\study\\_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    233\u001b[0m ):\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mc:\\Users\\stybl\\src\\acute-care-pathways\\systematic_comparison.ipynb Cell 18\u001b[0m in \u001b[0;36mObjective.__call__\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=36'>37</a>\u001b[0m trial_params \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=37'>38</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resampler\u001b[39m.\u001b[39msuggest_parameters(trial) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resampler \u001b[39melse\u001b[39;00m {}),\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=38'>39</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_estimator\u001b[39m.\u001b[39msuggest_parameters(trial),\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=39'>40</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=40'>41</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline_factory(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrial_params)\n\u001b[1;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=42'>43</a>\u001b[0m score \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=43'>44</a>\u001b[0m     model,\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=44'>45</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_X_train,\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=45'>46</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_y_train,\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=46'>47</a>\u001b[0m     cv\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cv,\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=47'>48</a>\u001b[0m     scoring\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scoring,\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=48'>49</a>\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mNotebook\u001b[39m.\u001b[39;49mCROSS_VAL_N_JOBS,\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=49'>50</a>\u001b[0m     fit_params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_params\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=50'>51</a>\u001b[0m )[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=52'>53</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persist_model \u001b[39mand\u001b[39;00m (score \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_best_score):\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=53'>54</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_best_score \u001b[39m=\u001b[39m score\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\imblearn\\pipeline.py:281\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m'\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    279\u001b[0m                          \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 281\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, yt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    282\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\stybl\\src\\acute-care-pathways\\systematic_comparison.ipynb Cell 18\u001b[0m in \u001b[0;36mTabNetWrapper.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[1;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000017vscode-remote?line=14'>15</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X_train\u001b[39m=\u001b[39;49mX\u001b[39m.\u001b[39;49mto_numpy(), y_train\u001b[39m=\u001b[39;49my\u001b[39m.\u001b[39;49mto_numpy(), eval_metric \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_metric, weights\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights, max_epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_epochs, patience\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatience, batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, virtual_batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvirtual_batch_size, drop_last\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdrop_last)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:223\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39mfor\u001b[39;00m epoch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs):\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m     \u001b[39m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[1;32m--> 223\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(train_dataloader)\n\u001b[0;32m    225\u001b[0m     \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:434\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m    432\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[1;32m--> 434\u001b[0m     batch_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(X, y)\n\u001b[0;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[0;32m    438\u001b[0m epoch_logs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]}\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:469\u001b[0m, in \u001b[0;36mTabModel._train_batch\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparameters():\n\u001b[0;32m    467\u001b[0m     param\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(X)\n\u001b[0;32m    471\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(output, y)\n\u001b[0;32m    472\u001b[0m \u001b[39m# Add the overall sparsity loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:583\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    582\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder(x)\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtabnet(x)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:468\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    467\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 468\u001b[0m     steps_output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    469\u001b[0m     res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mstack(steps_output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    471\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_multi_task:\n\u001b[0;32m    472\u001b[0m         \u001b[39m# Result will be in list format\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:160\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[1;34m(self, x, prior)\u001b[0m\n\u001b[0;32m    158\u001b[0m steps_output \u001b[39m=\u001b[39m []\n\u001b[0;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps):\n\u001b[1;32m--> 160\u001b[0m     M \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matt_transformers[step](prior, att)\n\u001b[0;32m    161\u001b[0m     M_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(\n\u001b[0;32m    162\u001b[0m         torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mmul(M, torch\u001b[39m.\u001b[39mlog(M \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon)), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m     \u001b[39m# update prior\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:635\u001b[0m, in \u001b[0;36mAttentiveTransformer.forward\u001b[1;34m(self, priors, processed_feat)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, priors, processed_feat):\n\u001b[0;32m    634\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(processed_feat)\n\u001b[1;32m--> 635\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x)\n\u001b[0;32m    636\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(x, priors)\n\u001b[0;32m    637\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselector(x)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:36\u001b[0m, in \u001b[0;36mGBN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     35\u001b[0m     chunks \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mchunk(\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvirtual_batch_size)), \u001b[39m0\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     res \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x_) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m chunks]\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(res, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     35\u001b[0m     chunks \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mchunk(\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvirtual_batch_size)), \u001b[39m0\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     res \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x_) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m chunks]\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(res, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    163\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    169\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    170\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    172\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    173\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    177\u001b[0m     bn_training,\n\u001b[0;32m    178\u001b[0m     exponential_average_factor,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    180\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\functional.py:2282\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2279\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2280\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2282\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2283\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2284\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study_grid = dict(\n",
    "    #estimator=[Estimator_LightGBM, Estimator_IsolationForest, Estimator_LogisticRegression, Estimator_RandomForest, Estimator_TabNet, Estimator_XGBoost],\n",
    "    estimator=[Estimator_TabNet],\n",
    "    resampler=[No_Resampling],\n",
    "    features=get_feature_studies(sci_train),\n",
    ")\n",
    "\n",
    "for _ in get_studies(sci_train, study_grid)[5:]:\n",
    "    s = construct_study(\n",
    "        **_, sci_train=SCIData(sci_train.head(1000)), sci_test=sci_test\n",
    "    )\n",
    "\n",
    "    r = s(n_trials=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Notebook.RUN:\n",
    "    studies = [\n",
    "        construct_study(**_, sci_train=sci_train, sci_test=sci_test)\n",
    "        for _ in get_studies(sci_train)\n",
    "    ]\n",
    "\n",
    "    print(\"Starting execution..\")\n",
    "    with parallel_backend(\"loky\", inner_max_num_threads=Notebook.CROSS_VAL_N_JOBS):\n",
    "        results = Parallel(n_jobs=Notebook.N_PROCESSES)(\n",
    "            delayed(_)(n_trials=Notebook.N_TRIALS, timeout=Notebook.TRIALS_TIMEOUT)\n",
    "            for _ in studies\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e35166262197b1ea9223463adaf11f6b58d81a82b7650a41ad4f3574b9c5682"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
