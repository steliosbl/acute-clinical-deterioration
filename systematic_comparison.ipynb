{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACP Project - Systematic Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import warnings, pickle, os, itertools\n",
    "from dataclasses import dataclass\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "try:\n",
    "    from sklearnex import patch_sklearn\n",
    "    patch_sklearn()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 300)\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "\n",
    "import shap\n",
    "import optuna, sqlalchemy\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import get_metrics, get_threshold_fpr\n",
    "%aimport utils.evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from dataset import SCIData, SCICols\n",
    "%aimport dataset\n",
    "\n",
    "sci = SCIData.load('data/sci.h5')\n",
    "\n",
    "scii = (\n",
    "    SCIData(SCIData.quickload(\"data/sci_processed.h5\").sort_values(\"AdmissionDateTime\"))\n",
    "    .mandate(SCICols.news_data_raw)\n",
    "    .derive_critical_event(within=1, return_subcols=True)\n",
    "    .augment_shmi(onehot=True)\n",
    "    .omit_redundant()\n",
    "    .raw_news()\n",
    "    .derive_ae_diagnosis_stems(onehot=False)\n",
    "    .categorize()\n",
    "   # .onehot_encode_categories()\n",
    ")\n",
    "\n",
    "sci_train, sci_test, _, y_test_mortality, _, y_test_criticalcare = train_test_split(\n",
    "    scii,\n",
    "    scii.DiedWithinThreshold,\n",
    "    scii.CriticalCare,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    "    shuffle=False,\n",
    ")\n",
    "sci_train, sci_test = SCIData(sci_train), SCIData(sci_test)\n",
    "# (X_train, y_train), (X_test, y_test) = (\n",
    "#     sci_train.xy(outcome=\"CriticalEvent\", dropna=False, fillna=False),\n",
    "#     sci_test.xy(outcome=\"CriticalEvent\", dropna=False, fillna=False),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = (\n",
    "    sci_train.xy(outcome=\"CriticalEvent\", dropna=False, fillna=False),\n",
    "    sci_test.xy(outcome=\"CriticalEvent\", dropna=False, fillna=False),\n",
    ")\n",
    "categorical_cols, categories = X_train.describe_categories()\n",
    "#X_train = X_train.ordinal_encode_categories()\n",
    "X_train = X_train.ordinal_encode_categories().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from typing import Dict, Any, Iterable\n",
    "\n",
    "\n",
    "class Estimator:\n",
    "    _name: str\n",
    "    _estimator: BaseEstimator\n",
    "    _requirements: Dict[str, bool]\n",
    "    _static_params: Dict[str, Any] = {}\n",
    "    _tuning_params_default: Dict[str, Any] = {}\n",
    "    _fit_params: Dict[str, Any] = {}\n",
    "\n",
    "    def __init__(self, sci_train):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        return dict()\n",
    "\n",
    "    @classmethod\n",
    "    def compile_parameters(cls, params):\n",
    "        return {\n",
    "            f\"{cls._name}__{key}\": value\n",
    "            for key, value in {\n",
    "                **cls._static_params,\n",
    "                **cls._tuning_params_default,\n",
    "                **params,\n",
    "            }.items()\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def factory(cls):\n",
    "        return cls._estimator(**cls._static_params)\n",
    "\n",
    "    @classmethod\n",
    "    def fit_params(cls, X_train, y_train):\n",
    "        return {f\"{cls._name}__{key}\": value for key, value in cls._fit_params.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn import FunctionSampler\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class Resampler(Estimator):\n",
    "    @classmethod\n",
    "    def compile_parameters(cls, params):\n",
    "        return {\n",
    "            f\"{cls._name}__kw_args\": {\n",
    "                **cls._static_params,\n",
    "                **cls._tuning_params_default,\n",
    "                **params,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def factory(cls):\n",
    "        return FunctionSampler(\n",
    "            func=partial(SCIData.resample, cls._estimator),\n",
    "            validate=False,\n",
    "            kw_args=cls._static_params,\n",
    "        )\n",
    "\n",
    "\n",
    "class Resampler_SMOTE(Resampler):\n",
    "    _name = \"SMOTE\"\n",
    "    _estimator = SMOTENC\n",
    "\n",
    "    _static_params = dict(random_state=42, n_jobs=None,)\n",
    "\n",
    "    _tuning_params_default = dict(sampling_strategy=0.1, k_neighbors=5)\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            sampling_strategy=trial.suggest_float(\n",
    "                f\"{cls._name}__sampling_strategy\", 0.1, 0.5\n",
    "            ),\n",
    "            k_neighbors=trial.suggest_int(f\"{cls._name}__k_neighbors\", 2, 10),\n",
    "        )\n",
    "\n",
    "        return cls.compile_parameters(suggestions)\n",
    "\n",
    "    @classmethod\n",
    "    def factory(cls):\n",
    "        return FunctionSampler(\n",
    "            func=SCIData.SMOTE, validate=False, kw_args=cls._static_params\n",
    "        )\n",
    "\n",
    "\n",
    "class Resampler_RandomUnderSampler(Resampler):\n",
    "    _name = \"RandomUnderSampler\"\n",
    "    _estimator = RandomUnderSampler\n",
    "\n",
    "    _static_params = dict(random_state=42, replacement=False)\n",
    "\n",
    "    _tuning_params_default = dict(sampling_strategy=0.1)\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            sampling_strategy=trial.suggest_float(\n",
    "                f\"{cls._name}__sampling_strategy\", 0.05, 0.5\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return cls.compile_parameters(suggestions)\n",
    "\n",
    "\n",
    "class No_Resampling(Resampler):\n",
    "    _name = \"No_Resampling\"\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        return dict()\n",
    "\n",
    "    @staticmethod\n",
    "    def _(X, y):\n",
    "        return X, y\n",
    "\n",
    "    @classmethod\n",
    "    def factory(cls):\n",
    "        return FunctionSampler(func=cls._, validate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "class Estimator_LightGBM(Estimator):\n",
    "    _name = \"LightGBM\"\n",
    "    _estimator = LGBMClassifier\n",
    "\n",
    "    _requirements = dict(\n",
    "        onehot=False, ordinal=False, imputation=False, fillna=False, resampling=False\n",
    "    )\n",
    "\n",
    "    _static_params = dict(\n",
    "        objective=\"binary\",\n",
    "        metric=[\"l2\", \"auc\"],\n",
    "        boosting_type=\"gbdt\",\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "    )\n",
    "\n",
    "    _tuning_params_default = dict(\n",
    "        is_unbalance=True,\n",
    "        reg_alpha=1.8e-3,\n",
    "        reg_lambda=6e-4,\n",
    "        num_leaves=14,\n",
    "        colsample_bytree=0.4,\n",
    "        subsample=0.97,\n",
    "        subsample_freq=1,\n",
    "        min_child_samples=6,\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            reg_alpha=trial.suggest_float(\n",
    "                f\"{cls._name}__reg_alpha\", 1e-4, 10.0, log=True\n",
    "            ),\n",
    "            reg_lambda=trial.suggest_float(\n",
    "                f\"{cls._name}__reg_lambda\", 1e-4, 10.0, log=True\n",
    "            ),\n",
    "            num_leaves=trial.suggest_int(f\"{cls._name}__num_leaves\", 2, 256),\n",
    "            colsample_bytree=trial.suggest_float(\n",
    "                f\"{cls._name}__colsample_bytree\", 0.4, 1.0\n",
    "            ),\n",
    "            subsample=trial.suggest_float(f\"{cls._name}__subsample\", 0.4, 1.0),\n",
    "            subsample_freq=trial.suggest_int(f\"{cls._name}__subsample_freq\", 1, 7),\n",
    "            min_child_samples=trial.suggest_int(\n",
    "                f\"{cls._name}__min_child_samples\", 5, 150\n",
    "            ),\n",
    "            is_unbalance=trial.suggest_categorical(\n",
    "                f\"{cls._name}__is_unbalance\", [True, False]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if not suggestions[\"is_unbalance\"]:\n",
    "            suggestions[\"scale_pos_weight\"] = trial.suggest_int(\n",
    "                f\"{cls._name}__scale_pos_weight\", 1, 100\n",
    "            )\n",
    "\n",
    "        r = cls.compile_parameters(suggestions)\n",
    "        if not suggestions[\"is_unbalance\"]:\n",
    "            del r[f\"{cls._name}__is_unbalance\"]\n",
    "\n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "class Estimator_XGBoost(Estimator):\n",
    "    _name = \"XGBoost\"\n",
    "    _estimator = XGBClassifier\n",
    "\n",
    "    _requirements = dict(\n",
    "        onehot=False, ordinal=False, imputation=False, fillna=False, resampling=False\n",
    "    )\n",
    "\n",
    "    _static_params = dict(\n",
    "        verbosity=0,\n",
    "        n_jobs=1,\n",
    "        objective=\"binary:logistic\",\n",
    "        booster=\"gbtree\",\n",
    "        enable_categorical=True,\n",
    "    )\n",
    "\n",
    "    _tuning_params_default = {\n",
    "        **dict(\n",
    "            tree_method=\"hist\",\n",
    "            alpha=7e-05,\n",
    "            subsample=0.42,\n",
    "            colsample_bytree=0.87,\n",
    "            scale_pos_weight=14,\n",
    "            max_depth=7,\n",
    "            min_child_weight=10,\n",
    "            eta=0.035,\n",
    "            gamma=4e-08,\n",
    "            grow_policy=\"lossguide\",\n",
    "        ),\n",
    "        \"lambda\": 7e-2,\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            tree_method=trial.suggest_categorical(\n",
    "                f\"{cls._name}__tree_method\", [\"approx\", \"hist\"]\n",
    "            ),\n",
    "            alpha=trial.suggest_float(f\"{cls._name}__alpha\", 1e-8, 1.0, log=True),\n",
    "            subsample=trial.suggest_float(f\"{cls._name}__subsample\", 0.2, 1.0),\n",
    "            colsample_bytree=trial.suggest_float(\n",
    "                f\"{cls._name}__colsample_bytree\", 0.2, 1.0\n",
    "            ),\n",
    "            scale_pos_weight=trial.suggest_int(\n",
    "                f\"{cls._name}__scale_pos_weight\", 1, 100\n",
    "            ),\n",
    "            max_depth=trial.suggest_int(f\"{cls._name}__max_depth\", 3, 9, step=2),\n",
    "            min_child_weight=trial.suggest_int(f\"{cls._name}__min_child_weight\", 2, 10),\n",
    "            eta=trial.suggest_float(f\"{cls._name}__eta\", 1e-8, 1.0, log=True),\n",
    "            gamma=trial.suggest_float(f\"{cls._name}__gamma\", 1e-8, 1.0, log=True),\n",
    "            grow_policy=trial.suggest_categorical(\n",
    "                f\"{cls._name}__grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "            ),\n",
    "        )\n",
    "        suggestions[\"lambda\"] = trial.suggest_float(\n",
    "            f\"{cls._name}__lambda\", 1e-8, 1.0, log=True\n",
    "        )\n",
    "\n",
    "        return cls.compile_parameters(suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class Estimator_LogisticRegression(Estimator):\n",
    "    _name = \"LogisticRegression\"\n",
    "    _estimator = LogisticRegression\n",
    "\n",
    "    _requirements = dict(\n",
    "        onehot=True, ordinal=False, imputation=True, fillna=True, resampling=False\n",
    "    )\n",
    "\n",
    "    _static_params = dict(max_iter=100, solver=\"lbfgs\", random_state=42, penalty=\"l2\")\n",
    "\n",
    "    _tuning_params_default = dict(penalty=\"l2\", C=5.9, class_weight=\"balanced\")\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            penalty=trial.suggest_categorical(f\"{cls._name}__penalty\", [\"l2\", \"none\"]),\n",
    "            C=trial.suggest_float(f\"{cls._name}__C\", 0.01, 10),\n",
    "            class_weight=trial.suggest_categorical(\n",
    "                f\"{cls._name}__class_weight\", [None, \"balanced\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # if suggestions[\"penalty\"] == \"elasticnet\":\n",
    "        #     suggestions[\"l1_ratio\"] = trial.suggest_float(\n",
    "        #         f\"{cls._name}__l1_ratio\", 0.05, 0.95\n",
    "        #     )\n",
    "\n",
    "        return cls.compile_parameters(suggestions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class Estimator_RandomForest(Estimator):\n",
    "    _estimator = RandomForestClassifier\n",
    "    _name = \"RandomForest\"\n",
    "\n",
    "    _requirements = dict(\n",
    "        onehot=False, ordinal=True, imputation=False, fillna=True, resampling=False\n",
    "    )\n",
    "    _tuning_params_default = dict(\n",
    "        n_estimators=250,\n",
    "        max_features=0.56,\n",
    "        min_samples_split=8,\n",
    "        min_samples_leaf=3,\n",
    "        max_samples=0.75,\n",
    "        class_weight=\"balanced\",\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            n_estimators=trial.suggest_int(f\"{cls._name}__n_estimators\", 25, 250),\n",
    "            max_features=trial.suggest_float(f\"{cls._name}__max_features\", 0.15, 1.0),\n",
    "            min_samples_split=trial.suggest_int(\n",
    "                f\"{cls._name}__min_samples_split\", 2, 15\n",
    "            ),\n",
    "            min_samples_leaf=trial.suggest_int(f\"{cls._name}__min_samples_leaf\", 1, 15),\n",
    "            max_samples=trial.suggest_float(f\"{cls._name}__max_samples\", 0.5, 0.99),\n",
    "            class_weight=trial.suggest_categorical(\n",
    "                f\"{cls._name}__class_weight\", [None, \"balanced\", \"balanced_subsample\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return cls.compile_parameters(suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.isolation_forest_wrapper import IsolationForestWrapper\n",
    "\n",
    "\n",
    "class Estimator_IsolationForest(Estimator):\n",
    "    _name = \"IsolationForest\"\n",
    "    _estimator = IsolationForestWrapper\n",
    "    _requirements = dict(\n",
    "        onehot=True, ordinal=False, imputation=True, fillna=True, resampling=False\n",
    "    )\n",
    "\n",
    "    _tuning_params_default = dict(\n",
    "        n_estimators=140,\n",
    "        max_samples=0.45,\n",
    "        contamination=0.02,\n",
    "        max_features=0.69,\n",
    "        bootstrap=False,\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            n_estimators=trial.suggest_int(f\"{cls._name}__n_estimators\", 1, 200),\n",
    "            max_samples=trial.suggest_float(f\"{cls._name}__max_samples\", 0.0, 1.0),\n",
    "            contamination=trial.suggest_float(\n",
    "                f\"{cls._name}__contamination\", 1e-6, 1e-1\n",
    "            ),\n",
    "            max_features=trial.suggest_float(f\"{cls._name}__max_features\", 0.0, 1.0),\n",
    "            bootstrap=trial.suggest_categorical(\n",
    "                f\"{cls._name}__bootstrap\", [True, False]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return cls.compile_parameters(suggestions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TabNetWrapper(TabNetClassifier):\n",
    "    weights: int = 0\n",
    "    max_epochs: int = 100\n",
    "    patience: int = 10\n",
    "    batch_size: int = 1024\n",
    "    virtual_batch_size: int = 128\n",
    "    drop_last: bool = True\n",
    "    eval_metric: str = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return super().fit(\n",
    "            X_train=X.to_numpy(),\n",
    "            y_train=y.to_numpy(),\n",
    "            eval_metric=self.eval_metric,\n",
    "            weights=self.weights,\n",
    "            max_epochs=self.max_epochs,\n",
    "            patience=self.patience,\n",
    "            batch_size=self.batch_size,\n",
    "            virtual_batch_size=self.virtual_batch_size,\n",
    "            drop_last=self.drop_last,\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        return super().predict(X.to_numpy())\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return super().predict_proba(X.to_numpy())\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return self.predict_proba(X)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator_TabNet(Estimator):\n",
    "    _estimator = TabNetWrapper\n",
    "    _name = \"TabNet\"\n",
    "    _requirements = dict(\n",
    "        onehot=False, ordinal=True, imputation=True, fillna=True, resampling=False\n",
    "    )\n",
    "\n",
    "    _static_params = dict(\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        verbose=0,\n",
    "        device_name=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        scheduler_params=dict(mode=\"min\", min_lr=1e-5, factor=0.5),\n",
    "        optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "        cat_emb_dim=1,\n",
    "        max_epochs=50,\n",
    "        eval_metric=\"average_precision\",\n",
    "        weights=1,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    _tuning_params_default = dict(\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.2,\n",
    "        lambda_sparse=8e-4,\n",
    "        mask_type=\"sparsemax\",\n",
    "        n_shared=3,\n",
    "        scheduler_params=dict(patience=5),\n",
    "    )\n",
    "\n",
    "    def __init__(self, sci_train):\n",
    "        self._categorical_idxs, self._categorical_dims = sci_train.describe_categories(\n",
    "            dimensions=True\n",
    "        )\n",
    "\n",
    "    def factory(self):\n",
    "        return self._estimator(\n",
    "            cat_idxs=self._categorical_idxs,\n",
    "            cat_dims=[\n",
    "                _ + 1 for _ in self._categorical_dims\n",
    "            ],  # Because we may add 1 category when we fill_na\n",
    "            **self._static_params,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            n_steps=trial.suggest_int(f\"{cls._name}__n_steps\", 1, 10),\n",
    "            n_shared=trial.suggest_int(f\"{cls._name}__n_shared\", 1, 10),\n",
    "            gamma=trial.suggest_float(f\"{cls._name}__gamma\", 1, 1.5),\n",
    "            lambda_sparse=trial.suggest_float(\n",
    "                f\"{cls._name}__lambda_sparse\", 1e-6, 1e-3, log=True\n",
    "            ),\n",
    "            mask_type=trial.suggest_categorical(\n",
    "                f\"{cls._name}__mask_type\", [\"entmax\", \"sparsemax\"]\n",
    "            ),\n",
    "            scheduler_params=dict(\n",
    "                patience=trial.suggest_int(f\"{cls._name}__scheduler__patience\", 3, 10)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        n_da = trial.suggest_int(f\"{cls._name}__n_da\", 4, 32,)\n",
    "        suggestions[\"n_d\"], suggestions[\"n_a\"] = n_da, n_da\n",
    "\n",
    "        return cls.compile_parameters(suggestions)\n",
    "\n",
    "    @classmethod\n",
    "    def compile_parameters(cls, params):\n",
    "        r = {\n",
    "            **cls._static_params,\n",
    "            **cls._tuning_params_default,\n",
    "            **params,\n",
    "            \"scheduler_params\": {\n",
    "                **cls._static_params[\"scheduler_params\"],\n",
    "                **params[\"scheduler_params\"],\n",
    "            },\n",
    "        }\n",
    "        return {f\"{cls._name}__{key}\": value for key, value in r.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_studies(sci_train):\n",
    "    news = SCICols.news_data_raw\n",
    "    news_extended = SCICols.news_data_raw + SCICols.news_data_extras\n",
    "    labs = SCICols.blood\n",
    "    hospital = [\n",
    "        \"AdmissionMethodDescription\",\n",
    "        \"AdmissionSpecialty\",\n",
    "        \"SentToSDEC\",\n",
    "        \"Readmission\",\n",
    "    ]\n",
    "    ae = [\"AandEPresentingComplaint\", \"AandEMainDiagnosis\"]\n",
    "    diagnoses = [_ for _ in sci_train.columns if _.startswith(\"SHMI__\")]\n",
    "    phenotype = [\"Female\", \"Age\"]\n",
    "\n",
    "    return list(\n",
    "        dict(\n",
    "            news=news,\n",
    "            news_extended=news_extended,\n",
    "            news_with_phenotype=news_extended + phenotype,\n",
    "            with_ae_notes=news_extended + phenotype + ae,\n",
    "            with_labs=news_extended + phenotype + labs,\n",
    "            with_notes_and_labs=news_extended + phenotype + ae + labs,\n",
    "            with_hospital=news_extended + phenotype + hospital,\n",
    "            with_notes_and_hospital=news_extended + phenotype + ae + hospital,\n",
    "            with_labs_and_hospital=news_extended + phenotype + labs + hospital,\n",
    "            with_labs_and_diagnoses=news_extended + phenotype + labs + diagnoses,\n",
    "            all=news_extended + phenotype + ae + labs + hospital + diagnoses,\n",
    "        ).items()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_studies(sci_train, study_grid=None, cli_model_arg=None):\n",
    "    estimators = dict(\n",
    "        cpu=[\n",
    "            Estimator_IsolationForest,\n",
    "            Estimator_LightGBM,\n",
    "            Estimator_LogisticRegression,\n",
    "            Estimator_RandomForest,\n",
    "            Estimator_XGBoost,\n",
    "        ],\n",
    "        gpu=[Estimator_TabNet],\n",
    "    )\n",
    "    estimators[\"all\"] = estimators[None] = estimators[\"cpu\"] + estimators[\"gpu\"]\n",
    "    estimators.update({\n",
    "        _._name: [_] for _ in estimators['all']\n",
    "    })\n",
    "\n",
    "    if study_grid is None:\n",
    "        study_grid = dict(\n",
    "            estimator=estimators[cli_model_arg],\n",
    "            resampler=[No_Resampling, Resampler_RandomUnderSampler, Resampler_SMOTE],\n",
    "            features=get_feature_studies(sci_train),\n",
    "        )\n",
    "\n",
    "    k, v = zip(*study_grid.items())\n",
    "    return [dict(zip(k, _)) for _ in itertools.product(*v)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from typing import Optional\n",
    "\n",
    "class PipelineFactory:\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator: Estimator,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        resampler: Optional[Estimator] = None,\n",
    "    ):\n",
    "        (self._estimator, self._resampler, self._X_train, self._y_train,) = (\n",
    "            estimator,\n",
    "            resampler,\n",
    "            X_train,\n",
    "            y_train,\n",
    "        )\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        steps = [\n",
    "            (self._estimator._name, self._estimator.factory(),),\n",
    "        ]\n",
    "        if self._resampler is not None:\n",
    "            steps = [(self._resampler._name, self._resampler.factory(),),] + steps\n",
    "\n",
    "        return ImbPipeline(steps=steps).set_params(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "class Objective:\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator: Estimator,\n",
    "        resampler: Estimator,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=\"average_precision\",\n",
    "        cv_jobs=1,\n",
    "        n_trials=100,\n",
    "        stop_callback=None\n",
    "    ):\n",
    "        (\n",
    "            self._estimator,\n",
    "            self._resampler,\n",
    "            self._X_train,\n",
    "            self._y_train,\n",
    "            self._cv,\n",
    "            self._scoring,\n",
    "            self._cv_jobs,\n",
    "            self._n_trials,\n",
    "            self._stop_callback\n",
    "        ) = (\n",
    "            estimator,\n",
    "            resampler,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv,\n",
    "            scoring,\n",
    "            cv_jobs,\n",
    "            n_trials,\n",
    "            stop_callback\n",
    "        )\n",
    "\n",
    "        self._best_score = 0\n",
    "        self._best_model = None\n",
    "\n",
    "        self._pipeline_factory = PipelineFactory(\n",
    "            estimator=self._estimator,\n",
    "            resampler=self._resampler,\n",
    "            X_train=self._X_train,\n",
    "            y_train=self._y_train,\n",
    "        )\n",
    "\n",
    "        self._fit_params = self._estimator.fit_params(self._X_train, self._y_train)\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        if trial.number >= self._n_trials:\n",
    "            self._stop_callback()\n",
    "        \n",
    "        trial_params = {\n",
    "            **(self._resampler.suggest_parameters(trial) if self._resampler else {}),\n",
    "            **self._estimator.suggest_parameters(trial),\n",
    "        }\n",
    "        model = self._pipeline_factory(**trial_params)\n",
    "\n",
    "        score = cross_validate(\n",
    "            model,\n",
    "            self._X_train,\n",
    "            self._y_train,\n",
    "            cv=self._cv,\n",
    "            scoring=self._scoring,\n",
    "            n_jobs=self._cv_jobs,\n",
    "            fit_params=self._fit_params,\n",
    "        )[\"test_score\"].mean()\n",
    "\n",
    "        if score > self._best_score:\n",
    "            self._best_score = score\n",
    "            self._best_model = self._pipeline_factory(**trial_params).fit(\n",
    "                self._X_train, self._y_train\n",
    "            )\n",
    "\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Iterable, Optional, Tuple\n",
    "\n",
    "\n",
    "def construct_study(\n",
    "    estimator: Estimator,\n",
    "    sci_train: SCIData,\n",
    "    sci_test: SCIData,\n",
    "    features: Tuple[str, Iterable[str]] = (\"All\", []),\n",
    "    resampler: Estimator = None,\n",
    "    cv=5,\n",
    "    scoring=\"average_precision\",\n",
    "    storage=None,\n",
    "    model_persistence_path=None,\n",
    "    cv_jobs=1,\n",
    "    n_trials=100,\n",
    "    **kwargs,\n",
    "):\n",
    "    sci_args = dict(\n",
    "        x=features[1],\n",
    "        imputation=estimator._requirements[\"imputation\"],\n",
    "        onehot_encoding=estimator._requirements[\"onehot\"],\n",
    "        ordinal_encoding=estimator._requirements[\"ordinal\"],\n",
    "        fillna=estimator._requirements[\"fillna\"],\n",
    "    )\n",
    "    (X_train, y_train), (X_test, y_test) = sci_train.xy(**sci_args), sci_test.xy(**sci_args)\n",
    "\n",
    "    name = f\"{estimator._name}_{resampler._name if resampler else 'None'}_{features[0]}\"\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=name, storage=storage, load_if_exists=True)\n",
    "    objective = Objective(\n",
    "        estimator=estimator(SCIData(sci_train[features[1]])),\n",
    "        resampler=resampler(SCIData(sci_train[features[1]])),\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        cv_jobs=cv_jobs,\n",
    "        n_trials=n_trials,\n",
    "        stop_callback=study.stop\n",
    "    )\n",
    "\n",
    "    def handle_study_result(model_persistence_path=None, n_resamples=99, **kwargs):\n",
    "        model = objective._best_model\n",
    "        if model_persistence_path is not None:\n",
    "            with open(f\"{model_persistence_path}/{name}\", \"wb\") as file:\n",
    "                pickle.dump(model, file)\n",
    "\n",
    "        try:\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        except AttributeError:\n",
    "            y_pred_proba = model.decision_function(X_test)\n",
    "            \n",
    "        y_pred = np.where(y_pred_proba > get_threshold_fpr(y_test, y_pred_proba, target=0.05), 1, 0)\n",
    "\n",
    "        metrics = {\n",
    "            **dict(\n",
    "                name=name,\n",
    "                estimator=estimator._name,\n",
    "                resampler=resampler._name,\n",
    "                features=features[0]\n",
    "            ),\n",
    "            **get_metrics(y_test, y_pred, y_pred_proba, n_resamples)\n",
    "        }\n",
    "\n",
    "        return metrics, (name, y_pred_proba)\n",
    "\n",
    "        \n",
    "\n",
    "    def call(model_persistence_path=None, n_resamples=99, **kwargs):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            while True:\n",
    "                try:\n",
    "                    study.optimize(objective, **kwargs)\n",
    "                    return handle_study_result(model_persistence_path, n_resamples)\n",
    "                except (optuna.exceptions.StorageInternalError, sqlalchemy.exc.OperationalError):\n",
    "                    print(\"################# CAUGHT DB ERROR #################\")\n",
    "                    pass\n",
    "\n",
    "    return call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = SCIData(sci_train.head(1000)).xy(imputation=False, fillna=True, onehot_encoding=False, ordinal_encoding=True)\n",
    "# #XX, yy = Resampler_RandomUnderSampler(sci_train).factory().fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "# study_grid = dict(\n",
    "#     # estimator=[Estimator_LightGBM, Estimator_IsolationForest, Estimator_LogisticRegression, Estimator_RandomForest, Estimator_TabNet, Estimator_XGBoost],\n",
    "#     estimator=[Estimator_LightGBM],\n",
    "#     resampler=[No_Resampling, Resampler_SMOTE, Resampler_RandomUnderSampler],\n",
    "#     features=get_feature_studies(sci_train),\n",
    "# )\n",
    "\n",
    "# for _ in get_studies(sci_train, study_grid):\n",
    "#     s = construct_study(**_, sci_train=SCIData(sci_train.head(1000)), sci_test=SCIData(sci_test.head(1000)))\n",
    "\n",
    "#     r = s(n_trials=2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "# studies = [construct_study(**_, storage='sqlite:///models/studies.db', n_trials=2, sci_train=SCIData(sci_train.head(1000)), sci_test=SCIData(sci_test.head(1000))) for _ in get_studies(sci_train, study_grid)[:5]]\n",
    "# with parallel_backend(\"loky\", inner_max_num_threads=1):\n",
    "#             results = Parallel(n_jobs=1)(\n",
    "#                 delayed(_)(n_trials=2) for _ in studies[:2]\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-07 09:44:16,482]\u001b[0m A new study created in memory with name: LightGBM_No_Resampling_news\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:16,496]\u001b[0m A new study created in memory with name: LightGBM_No_Resampling_news_extended\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:16,516]\u001b[0m A new study created in memory with name: LightGBM_No_Resampling_news_with_phenotype\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:16,535]\u001b[0m A new study created in memory with name: LightGBM_No_Resampling_with_ae_notes\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:16,560]\u001b[0m A new study created in memory with name: LightGBM_No_Resampling_with_labs\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:16,587]\u001b[0m A new study created in memory with name: LightGBM_No_Resampling_with_notes_and_labs\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:16,607]\u001b[0m A new study created in memory with name: LightGBM_No_Resampling_with_hospital\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:16,631]\u001b[0m A new study created in memory with name: LightGBM_No_Resampling_with_notes_and_hospital\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:16,658]\u001b[0m A new study created in memory with name: LightGBM_No_Resampling_with_labs_and_hospital\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:16,829]\u001b[0m A new study created in memory with name: LightGBM_No_Resampling_with_labs_and_diagnoses\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:16,999]\u001b[0m A new study created in memory with name: LightGBM_No_Resampling_all\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,010]\u001b[0m A new study created in memory with name: LightGBM_RandomUnderSampler_news\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,025]\u001b[0m A new study created in memory with name: LightGBM_RandomUnderSampler_news_extended\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,051]\u001b[0m A new study created in memory with name: LightGBM_RandomUnderSampler_news_with_phenotype\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,077]\u001b[0m A new study created in memory with name: LightGBM_RandomUnderSampler_with_ae_notes\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,110]\u001b[0m A new study created in memory with name: LightGBM_RandomUnderSampler_with_labs\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,139]\u001b[0m A new study created in memory with name: LightGBM_RandomUnderSampler_with_notes_and_labs\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,167]\u001b[0m A new study created in memory with name: LightGBM_RandomUnderSampler_with_hospital\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,195]\u001b[0m A new study created in memory with name: LightGBM_RandomUnderSampler_with_notes_and_hospital\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,227]\u001b[0m A new study created in memory with name: LightGBM_RandomUnderSampler_with_labs_and_hospital\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,439]\u001b[0m A new study created in memory with name: LightGBM_RandomUnderSampler_with_labs_and_diagnoses\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,626]\u001b[0m A new study created in memory with name: LightGBM_RandomUnderSampler_all\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,645]\u001b[0m A new study created in memory with name: LightGBM_SMOTE_news\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,667]\u001b[0m A new study created in memory with name: LightGBM_SMOTE_news_extended\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,697]\u001b[0m A new study created in memory with name: LightGBM_SMOTE_news_with_phenotype\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,731]\u001b[0m A new study created in memory with name: LightGBM_SMOTE_with_ae_notes\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,779]\u001b[0m A new study created in memory with name: LightGBM_SMOTE_with_labs\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,816]\u001b[0m A new study created in memory with name: LightGBM_SMOTE_with_notes_and_labs\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,839]\u001b[0m A new study created in memory with name: LightGBM_SMOTE_with_hospital\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,864]\u001b[0m A new study created in memory with name: LightGBM_SMOTE_with_notes_and_hospital\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:17,894]\u001b[0m A new study created in memory with name: LightGBM_SMOTE_with_labs_and_hospital\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:18,050]\u001b[0m A new study created in memory with name: LightGBM_SMOTE_with_labs_and_diagnoses\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:18,220]\u001b[0m A new study created in memory with name: LightGBM_SMOTE_all\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:18,307]\u001b[0m Trial 0 finished with value: 0.15776589450226286 and parameters: {'LightGBM__reg_alpha': 0.02164240630738049, 'LightGBM__reg_lambda': 0.1921471556650901, 'LightGBM__num_leaves': 70, 'LightGBM__colsample_bytree': 0.4059661736152634, 'LightGBM__subsample': 0.4973600141135704, 'LightGBM__subsample_freq': 4, 'LightGBM__min_child_samples': 120, 'LightGBM__is_unbalance': False, 'LightGBM__scale_pos_weight': 92}. Best is trial 0 with value: 0.15776589450226286.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution (linear)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-07 09:44:18,449]\u001b[0m Trial 1 finished with value: 0.29946450255209417 and parameters: {'LightGBM__reg_alpha': 0.0002763158093743723, 'LightGBM__reg_lambda': 3.063653482058345, 'LightGBM__num_leaves': 203, 'LightGBM__colsample_bytree': 0.4837342364126186, 'LightGBM__subsample': 0.6871544403776597, 'LightGBM__subsample_freq': 2, 'LightGBM__min_child_samples': 22, 'LightGBM__is_unbalance': True}. Best is trial 1 with value: 0.29946450255209417.\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:20,823]\u001b[0m Trial 0 finished with value: 0.3703555322118333 and parameters: {'LightGBM__reg_alpha': 0.409743676844804, 'LightGBM__reg_lambda': 0.3984365038412139, 'LightGBM__num_leaves': 153, 'LightGBM__colsample_bytree': 0.5329607121314955, 'LightGBM__subsample': 0.8917526517623172, 'LightGBM__subsample_freq': 2, 'LightGBM__min_child_samples': 105, 'LightGBM__is_unbalance': True}. Best is trial 0 with value: 0.3703555322118333.\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:20,939]\u001b[0m Trial 1 finished with value: 0.3553002361564913 and parameters: {'LightGBM__reg_alpha': 1.4654338083553053, 'LightGBM__reg_lambda': 0.00667299117945733, 'LightGBM__num_leaves': 15, 'LightGBM__colsample_bytree': 0.9043490679325192, 'LightGBM__subsample': 0.8595410532861434, 'LightGBM__subsample_freq': 2, 'LightGBM__min_child_samples': 146, 'LightGBM__is_unbalance': False, 'LightGBM__scale_pos_weight': 53}. Best is trial 0 with value: 0.3703555322118333.\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:23,321]\u001b[0m Trial 0 finished with value: 0.39153121372212385 and parameters: {'LightGBM__reg_alpha': 5.846556872123371, 'LightGBM__reg_lambda': 0.07546247918193161, 'LightGBM__num_leaves': 11, 'LightGBM__colsample_bytree': 0.6938712546887251, 'LightGBM__subsample': 0.7573922143533883, 'LightGBM__subsample_freq': 3, 'LightGBM__min_child_samples': 54, 'LightGBM__is_unbalance': True}. Best is trial 0 with value: 0.39153121372212385.\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:23,451]\u001b[0m Trial 1 finished with value: 0.30861165790767103 and parameters: {'LightGBM__reg_alpha': 0.0008141379799559103, 'LightGBM__reg_lambda': 0.00016078239785169153, 'LightGBM__num_leaves': 42, 'LightGBM__colsample_bytree': 0.7958144768467363, 'LightGBM__subsample': 0.6600449411188428, 'LightGBM__subsample_freq': 2, 'LightGBM__min_child_samples': 88, 'LightGBM__is_unbalance': True}. Best is trial 0 with value: 0.39153121372212385.\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:25,732]\u001b[0m Trial 0 finished with value: 0.343565356916529 and parameters: {'LightGBM__reg_alpha': 0.22099431745370166, 'LightGBM__reg_lambda': 2.6598927578722575, 'LightGBM__num_leaves': 3, 'LightGBM__colsample_bytree': 0.6706524027888228, 'LightGBM__subsample': 0.982652609771477, 'LightGBM__subsample_freq': 3, 'LightGBM__min_child_samples': 5, 'LightGBM__is_unbalance': True}. Best is trial 0 with value: 0.343565356916529.\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:25,943]\u001b[0m Trial 1 finished with value: 0.3561529897951604 and parameters: {'LightGBM__reg_alpha': 0.00021348025359990228, 'LightGBM__reg_lambda': 0.0010363726108292652, 'LightGBM__num_leaves': 210, 'LightGBM__colsample_bytree': 0.6371941915328582, 'LightGBM__subsample': 0.8244496124758376, 'LightGBM__subsample_freq': 6, 'LightGBM__min_child_samples': 61, 'LightGBM__is_unbalance': True}. Best is trial 1 with value: 0.3561529897951604.\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:28,407]\u001b[0m Trial 0 finished with value: 0.28788267878852586 and parameters: {'LightGBM__reg_alpha': 0.1289569968881204, 'LightGBM__reg_lambda': 2.131653036084845, 'LightGBM__num_leaves': 236, 'LightGBM__colsample_bytree': 0.5824552421900855, 'LightGBM__subsample': 0.4463201344377003, 'LightGBM__subsample_freq': 4, 'LightGBM__min_child_samples': 81, 'LightGBM__is_unbalance': True}. Best is trial 0 with value: 0.28788267878852586.\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:28,595]\u001b[0m Trial 1 finished with value: 0.3147872380904918 and parameters: {'LightGBM__reg_alpha': 0.01787599267238935, 'LightGBM__reg_lambda': 0.008380546485360685, 'LightGBM__num_leaves': 230, 'LightGBM__colsample_bytree': 0.991231019806018, 'LightGBM__subsample': 0.7186424942785077, 'LightGBM__subsample_freq': 7, 'LightGBM__min_child_samples': 119, 'LightGBM__is_unbalance': True}. Best is trial 1 with value: 0.3147872380904918.\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:30,932]\u001b[0m Trial 0 finished with value: 0.32193642247938026 and parameters: {'LightGBM__reg_alpha': 0.00097934260690751, 'LightGBM__reg_lambda': 2.301566142229426, 'LightGBM__num_leaves': 180, 'LightGBM__colsample_bytree': 0.7969238595947042, 'LightGBM__subsample': 0.5118594790160848, 'LightGBM__subsample_freq': 5, 'LightGBM__min_child_samples': 90, 'LightGBM__is_unbalance': False, 'LightGBM__scale_pos_weight': 43}. Best is trial 0 with value: 0.32193642247938026.\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 09:44:31,092]\u001b[0m Trial 1 finished with value: 0.2674641373743346 and parameters: {'LightGBM__reg_alpha': 0.24411921167821468, 'LightGBM__reg_lambda': 2.5299448542943437, 'LightGBM__num_leaves': 57, 'LightGBM__colsample_bytree': 0.6004779609604063, 'LightGBM__subsample': 0.9697797616657777, 'LightGBM__subsample_freq': 3, 'LightGBM__min_child_samples': 110, 'LightGBM__is_unbalance': True}. Best is trial 0 with value: 0.32193642247938026.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\stybl\\src\\acute-care-pathways\\systematic_comparison.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 114>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=109'>110</a>\u001b[0m     pd\u001b[39m.\u001b[39mDataFrame(metrics)\u001b[39m.\u001b[39mto_hdf(args[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=110'>111</a>\u001b[0m     pd\u001b[39m.\u001b[39mDataFrame(\u001b[39mdict\u001b[39m(y_preds))\u001b[39m.\u001b[39mto_hdf(args[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39my_preds\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=113'>114</a>\u001b[0m run(args)\n",
      "\u001b[1;32mc:\\Users\\stybl\\src\\acute-care-pathways\\systematic_comparison.ipynb Cell 23\u001b[0m in \u001b[0;36mrun\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=104'>105</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=105'>106</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting execution (linear)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=106'>107</a>\u001b[0m     results \u001b[39m=\u001b[39m [_(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstudy_args) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m studies]\n\u001b[0;32m    <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=108'>109</a>\u001b[0m metrics, y_preds \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mresults))\n\u001b[0;32m    <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=109'>110</a>\u001b[0m pd\u001b[39m.\u001b[39mDataFrame(metrics)\u001b[39m.\u001b[39mto_hdf(args[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\stybl\\src\\acute-care-pathways\\systematic_comparison.ipynb Cell 23\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=104'>105</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=105'>106</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting execution (linear)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=106'>107</a>\u001b[0m     results \u001b[39m=\u001b[39m [_(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstudy_args) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m studies]\n\u001b[0;32m    <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=108'>109</a>\u001b[0m metrics, y_preds \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mresults))\n\u001b[0;32m    <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=109'>110</a>\u001b[0m pd\u001b[39m.\u001b[39mDataFrame(metrics)\u001b[39m.\u001b[39mto_hdf(args[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\stybl\\src\\acute-care-pathways\\systematic_comparison.ipynb Cell 23\u001b[0m in \u001b[0;36mconstruct_study.<locals>.call\u001b[1;34m(model_persistence_path, n_resamples, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=71'>72</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=72'>73</a>\u001b[0m     study\u001b[39m.\u001b[39moptimize(objective, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=73'>74</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_study_result(model_persistence_path, n_resamples)\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=74'>75</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (optuna\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mStorageInternalError, sqlalchemy\u001b[39m.\u001b[39mexc\u001b[39m.\u001b[39mOperationalError):\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=75'>76</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m################# CAUGHT DB ERROR #################\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\stybl\\src\\acute-care-pathways\\systematic_comparison.ipynb Cell 23\u001b[0m in \u001b[0;36mconstruct_study.<locals>.handle_study_result\u001b[1;34m(model_persistence_path, n_resamples, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=49'>50</a>\u001b[0m     y_pred_proba \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecision_function(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=51'>52</a>\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(y_pred_proba \u001b[39m>\u001b[39m get_threshold_fpr(y_test, y_pred_proba, target\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m), \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=53'>54</a>\u001b[0m metrics \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=54'>55</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=55'>56</a>\u001b[0m         name\u001b[39m=\u001b[39mname,\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=56'>57</a>\u001b[0m         estimator\u001b[39m=\u001b[39mestimator\u001b[39m.\u001b[39m_name,\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=57'>58</a>\u001b[0m         resampler\u001b[39m=\u001b[39mresampler\u001b[39m.\u001b[39m_name,\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=58'>59</a>\u001b[0m         features\u001b[39m=\u001b[39mfeatures[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=59'>60</a>\u001b[0m     ),\n\u001b[1;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=60'>61</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mget_metrics(y_test, y_pred, y_pred_proba, n_resamples)\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=61'>62</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstybl-acp/c%3A/Users/stybl/src/acute-care-pathways/systematic_comparison.ipynb#ch0000019vscode-remote?line=63'>64</a>\u001b[0m \u001b[39mreturn\u001b[39;00m metrics, (name, y_pred_proba)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\src\\acute-care-pathways\\utils\\evaluation.py:726\u001b[0m, in \u001b[0;36mget_metrics\u001b[1;34m(y_true, y_pred, y_pred_proba, n_resamples)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_metrics\u001b[39m(y_true, y_pred, y_pred_proba, n_resamples\u001b[39m=\u001b[39m\u001b[39m99\u001b[39m):\n\u001b[1;32m--> 726\u001b[0m     auc_lower, auc_upper \u001b[39m=\u001b[39m roc_auc_ci_bootstrap(y_true, y_pred_proba, n_resamples)\n\u001b[0;32m    727\u001b[0m     ap_lower, ap_upper \u001b[39m=\u001b[39m average_precision_ci_bootstrap(\n\u001b[0;32m    728\u001b[0m         y_true, y_pred_proba, n_resamples\n\u001b[0;32m    729\u001b[0m     )\n\u001b[0;32m    730\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(\n\u001b[0;32m    731\u001b[0m         AUC\u001b[39m=\u001b[39mroc_auc_score(y_true, y_pred_proba),\n\u001b[0;32m    732\u001b[0m         AUC_Upper\u001b[39m=\u001b[39mauc_upper,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    740\u001b[0m         F2\u001b[39m=\u001b[39mfbeta_score(y_true, y_pred, beta\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[0;32m    741\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\stybl\\src\\acute-care-pathways\\utils\\evaluation.py:407\u001b[0m, in \u001b[0;36mroc_auc_ci_bootstrap\u001b[1;34m(y_true, y_score, n_resamples)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mroc_auc_ci_bootstrap\u001b[39m(y_true, y_score, n_resamples\u001b[39m=\u001b[39m\u001b[39m9999\u001b[39m):\n\u001b[0;32m    406\u001b[0m     \u001b[39m\"\"\" Computes AUROC with 95% confidence intervals by boostrapping \"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[39mreturn\u001b[39;00m bootstrap_metric(roc_auc_score, y_true, y_score, n_resamples)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\src\\acute-care-pathways\\utils\\evaluation.py:391\u001b[0m, in \u001b[0;36mbootstrap_metric\u001b[1;34m(metric, y_true, y_score, n_resamples)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbootstrap_metric\u001b[39m(metric, y_true, y_score, n_resamples\u001b[39m=\u001b[39m\u001b[39m9999\u001b[39m):\n\u001b[0;32m    390\u001b[0m     \u001b[39m\"\"\" Computes AUROC with 95% confidence intervals by boostrapping \"\"\"\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m     res \u001b[39m=\u001b[39m st\u001b[39m.\u001b[39;49mbootstrap(\n\u001b[0;32m    392\u001b[0m         data\u001b[39m=\u001b[39;49m(y_true\u001b[39m.\u001b[39;49mto_numpy(), y_score),\n\u001b[0;32m    393\u001b[0m         statistic\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m    394\u001b[0m         confidence_level\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m,\n\u001b[0;32m    395\u001b[0m         method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpercentile\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    396\u001b[0m         n_resamples\u001b[39m=\u001b[39;49mn_resamples,\n\u001b[0;32m    397\u001b[0m         vectorized\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    398\u001b[0m         paired\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    399\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m,\n\u001b[0;32m    400\u001b[0m     )\n\u001b[0;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39mconfidence_interval\u001b[39m.\u001b[39mlow, res\u001b[39m.\u001b[39mconfidence_interval\u001b[39m.\u001b[39mhigh\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\scipy\\stats\\_bootstrap.py:465\u001b[0m, in \u001b[0;36mbootstrap\u001b[1;34m(data, statistic, vectorized, paired, axis, confidence_level, n_resamples, batch, method, random_state)\u001b[0m\n\u001b[0;32m    462\u001b[0m         resampled_data\u001b[39m.\u001b[39mappend(resample)\n\u001b[0;32m    464\u001b[0m     \u001b[39m# Compute bootstrap distribution of statistic\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     theta_hat_b\u001b[39m.\u001b[39mappend(statistic(\u001b[39m*\u001b[39;49mresampled_data, axis\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m    466\u001b[0m theta_hat_b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(theta_hat_b, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    468\u001b[0m \u001b[39m# Calculate percentile interval\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\scipy\\stats\\_bootstrap.py:178\u001b[0m, in \u001b[0;36m_bootstrap_iv.<locals>.statistic\u001b[1;34m(i, axis, data, unpaired_statistic)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstatistic\u001b[39m(i, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, data\u001b[39m=\u001b[39mdata_iv, unpaired_statistic\u001b[39m=\u001b[39mstatistic):\n\u001b[0;32m    177\u001b[0m     data \u001b[39m=\u001b[39m [sample[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, i] \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data]\n\u001b[1;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m unpaired_statistic(\u001b[39m*\u001b[39;49mdata, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\scipy\\stats\\_bootstrap.py:37\u001b[0m, in \u001b[0;36m_vectorize_statistic.<locals>.stat_nd\u001b[1;34m(axis, *data)\u001b[0m\n\u001b[0;32m     34\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msplit(z, split_indices)\n\u001b[0;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m statistic(\u001b[39m*\u001b[39mdata)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mapply_along_axis(stat_1d, axis, z)[()]\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\numpy\\lib\\shape_base.py:402\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m buff[ind0] \u001b[39m=\u001b[39m res\n\u001b[0;32m    401\u001b[0m \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m inds:\n\u001b[1;32m--> 402\u001b[0m     buff[ind] \u001b[39m=\u001b[39m asanyarray(func1d(inarr_view[ind], \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[0;32m    404\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(res, matrix):\n\u001b[0;32m    405\u001b[0m     \u001b[39m# wrap the array, to preserve subclasses\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     buff \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39m__array_wrap__(buff)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\scipy\\stats\\_bootstrap.py:35\u001b[0m, in \u001b[0;36m_vectorize_statistic.<locals>.stat_nd.<locals>.stat_1d\u001b[1;34m(z)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstat_1d\u001b[39m(z):\n\u001b[0;32m     34\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msplit(z, split_indices)\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m statistic(\u001b[39m*\u001b[39;49mdata)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:571\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    569\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y_true)\n\u001b[0;32m    570\u001b[0m     y_true \u001b[39m=\u001b[39m label_binarize(y_true, classes\u001b[39m=\u001b[39mlabels)[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m--> 571\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    572\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39;49mmax_fpr),\n\u001b[0;32m    573\u001b[0m         y_true,\n\u001b[0;32m    574\u001b[0m         y_score,\n\u001b[0;32m    575\u001b[0m         average,\n\u001b[0;32m    576\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    577\u001b[0m     )\n\u001b[0;32m    578\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m    579\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    580\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39mmax_fpr),\n\u001b[0;32m    581\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    584\u001b[0m         sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    585\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\sklearn\\metrics\\_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m binary_metric(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m     78\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:344\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_true)) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    340\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not defined in that case.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m     )\n\u001b[1;32m--> 344\u001b[0m fpr, tpr, _ \u001b[39m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m max_fpr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m max_fpr \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m auc(fpr, tpr)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:981\u001b[0m, in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mroc_curve\u001b[39m(\n\u001b[0;32m    893\u001b[0m     y_true, y_score, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, drop_intermediate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    894\u001b[0m ):\n\u001b[0;32m    895\u001b[0m     \u001b[39m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \n\u001b[0;32m    897\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    979\u001b[0m \n\u001b[0;32m    980\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[0;32m    982\u001b[0m         y_true, y_score, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[0;32m    983\u001b[0m     )\n\u001b[0;32m    985\u001b[0m     \u001b[39m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[0;32m    986\u001b[0m     \u001b[39m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[0;32m    987\u001b[0m     \u001b[39m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[39m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[0;32m    993\u001b[0m     \u001b[39m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[0;32m    994\u001b[0m     \u001b[39mif\u001b[39;00m drop_intermediate \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(fps) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:763\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    760\u001b[0m y_true \u001b[39m=\u001b[39m y_true \u001b[39m==\u001b[39m pos_label\n\u001b[0;32m    762\u001b[0m \u001b[39m# sort scores and corresponding truth values\u001b[39;00m\n\u001b[1;32m--> 763\u001b[0m desc_score_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margsort(y_score, kind\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmergesort\u001b[39;49m\u001b[39m\"\u001b[39;49m)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    764\u001b[0m y_score \u001b[39m=\u001b[39m y_score[desc_score_indices]\n\u001b[0;32m    765\u001b[0m y_true \u001b[39m=\u001b[39m y_true[desc_score_indices]\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margsort\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1120\u001b[0m, in \u001b[0;36margsort\u001b[1;34m(a, axis, kind, order)\u001b[0m\n\u001b[0;32m   1012\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[0;32m   1013\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39margsort\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1014\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[39m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \n\u001b[0;32m   1119\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margsort\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, kind\u001b[39m=\u001b[39;49mkind, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[1;32mc:\\Users\\stybl\\miniconda3\\envs\\py39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "if SCRIPT:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"-m\", \"--models\", help=\"Can be 'all', 'cpu', or 'gpu'\", type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-j\", \"--njobs\", help=\"Number of CPUs to use. Default=1\", type=int, default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-c\", \"--cv_jobs\", help=\"Number of CV jobs. Default=5\", type=int, default=5\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--persist\",\n",
    "        help=\"Filepath to save the models. If unset, wont save them\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--debug\",\n",
    "        help=\"Whether to only use a small subset of data for debugging\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-t\", \"--trials\", help=\"Number of trials. Default=1000\", type=int, default=1000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-hr\", \"--hours\", help=\"Trial timeout in hours\", type=int, default=2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--storage\",\n",
    "        help=\"Trial storage for optuna\",\n",
    "        default=\"sqlite:///models/studies.db\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-o\",\n",
    "        \"--output\",\n",
    "        help='Output path for final results',\n",
    "        default='results.h5'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_resamples',\n",
    "        help='Number of resamples for bootstrapping metrics',\n",
    "        default=999\n",
    "    )\n",
    "    parser.add_argument(\"-v\", \"--verbose\", help=\"Optuna verbosity\", action=\"store_true\")\n",
    "\n",
    "    args = vars(parser.parse_args())\n",
    "else:\n",
    "    args = dict(\n",
    "        models=\"LightGBM\",\n",
    "        njobs=1,\n",
    "        cvjobs=1,\n",
    "        persist=None,\n",
    "        debug=True,\n",
    "        trials=2,\n",
    "        hours=2,\n",
    "        storage=None,\n",
    "        verbose=True,\n",
    "        output='results.h5',\n",
    "        n_resamples=99,\n",
    "    )\n",
    "\n",
    "\n",
    "def run(args):\n",
    "    if args[\"verbose\"]:\n",
    "        optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "    if args[\"persist\"] is not None:\n",
    "        try:\n",
    "            os.makedirs(args[\"persist\"])\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "    sci_train_ = sci_train\n",
    "    if args[\"debug\"]:\n",
    "        sci_train_ = SCIData(sci_train.sample(1000))\n",
    "\n",
    "    if args['storage'] is not None:        \n",
    "        args['storage'] = optuna.storages.RDBStorage(url=args['storage'], engine_kwargs={\"connect_args\": {\"timeout\": 100}})\n",
    "    \n",
    "    n_trials = args[\"trials\"] if not args[\"debug\"] else 2\n",
    "\n",
    "    studies = [\n",
    "        construct_study(**_, **args, sci_train=sci_train_, sci_test=sci_test, n_trials=n_trials)\n",
    "        for _ in get_studies(sci_train, cli_model_arg=args[\"models\"])\n",
    "    ]\n",
    "\n",
    "    study_args = dict(\n",
    "        model_persistence_path=args[\"persist\"],\n",
    "        n_resamples=args['n_resamples'],\n",
    "        n_trials=n_trials,\n",
    "        timeout=args[\"hours\"] * 60 * 60,\n",
    "    )\n",
    "\n",
    "    if args[\"njobs\"] > 1:\n",
    "        print(\"Starting execution (parallel)\")\n",
    "        with parallel_backend(\"loky\", inner_max_num_threads=args[\"cv_jobs\"]):\n",
    "            results = Parallel(n_jobs=args[\"njobs\"])(\n",
    "                delayed(_)(**study_args) for _ in studies\n",
    "            )\n",
    "    else:\n",
    "        print(\"Starting execution (linear)\")\n",
    "        results = [_(**study_args) for _ in studies]\n",
    "\n",
    "    metrics, y_preds = list(zip(*results))\n",
    "    pd.DataFrame(metrics).to_hdf(args['output'], 'metrics')\n",
    "    pd.DataFrame(dict(y_preds)).to_hdf(args['output'], 'y_preds')\n",
    "\n",
    "\n",
    "run(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e35166262197b1ea9223463adaf11f6b58d81a82b7650a41ad4f3574b9c5682"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
