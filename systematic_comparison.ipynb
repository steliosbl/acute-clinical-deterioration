{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACP Project - Systematic Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, pickle, os, itertools\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "try:\n",
    "    from sklearnex import patch_sklearn\n",
    "    patch_sklearn()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 300)\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "\n",
    "import shap\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Notebook:\n",
    "    CROSS_VAL_N_JOBS = 5\n",
    "    N_PROCESSES = 5\n",
    "    N_TRIALS = 1000\n",
    "    TRIALS_TIMEOUT = 60*60*2\n",
    "    TRIAL_STORAGE = \"sqlite:///models/studies.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from dataset import SCIData, SCICols\n",
    "%aimport dataset\n",
    "\n",
    "sci = SCIData.load('data/sci.h5')\n",
    "\n",
    "scii = (\n",
    "    SCIData(SCIData.quickload(\"data/sci_processed.h5\").sort_values(\"AdmissionDateTime\"))\n",
    "    .mandate(SCICols.news_data_raw)\n",
    "    .derive_critical_event(within=1, return_subcols=True)\n",
    "    .augment_shmi(onehot=True)\n",
    "    .omit_redundant()\n",
    "    .raw_news()\n",
    "    .derive_ae_diagnosis_stems(onehot=False)\n",
    "    .categorize()\n",
    "   # .onehot_encode_categories()\n",
    ")\n",
    "\n",
    "sci_train, sci_test, _, y_test_mortality, _, y_test_criticalcare = train_test_split(\n",
    "    scii,\n",
    "    scii.DiedWithinThreshold,\n",
    "    scii.CriticalCare,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    "    shuffle=False,\n",
    ")\n",
    "sci_train, sci_test = SCIData(sci_train), SCIData(sci_test)\n",
    "# (X_train, y_train), (X_test, y_test) = (\n",
    "#     sci_train.xy(outcome=\"CriticalEvent\", dropna=False, fillna=False),\n",
    "#     sci_test.xy(outcome=\"CriticalEvent\", dropna=False, fillna=False),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from typing import Dict, Any, Iterable\n",
    "\n",
    "class Model(ABC):\n",
    "    _name: str\n",
    "    _estimator: BaseEstimator\n",
    "    _requirements: Dict[str, bool]\n",
    "    _static_params: Dict[str, Any]\n",
    "    _tuning_params_default: Dict[str, Any]\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        return dict()\n",
    "\n",
    "    @classmethod\n",
    "    def compile_params(cls, params):\n",
    "        return {\n",
    "            f'{cls._name}__{key}': value for key, value in {\n",
    "                **cls._static_params,\n",
    "                **cls._tuning_params_default,\n",
    "                **params\n",
    "            }.items()\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def get(cls, X_train, y_train):\n",
    "        return cls._estimator(**cls._static_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn import FunctionSampler\n",
    "\n",
    "class Resampler_SMOTE:\n",
    "    _name = 'SMOTE'\n",
    "    _estimator = SMOTENC\n",
    "\n",
    "    _static_params = dict(\n",
    "        random_state=42,\n",
    "        n_jobs=None,\n",
    "    )\n",
    "\n",
    "    _tuning_params_default = dict(\n",
    "        sampling_strategy=0.1,\n",
    "        k_neighbors=5\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            sampling_strategy = trial.suggest_float(\n",
    "                f'{cls._name}__sampling_strategy', 0.1, 0.5\n",
    "            ),\n",
    "            k_neighbors = trial.suggest_int(\n",
    "                f'{cls._name}__k_neighbors', 2, 10\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            f'{cls._name}__kw_args': {\n",
    "                **cls._static_params,\n",
    "                **cls._tuning_params_default,\n",
    "                **suggestions\n",
    "            } \n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, X_train, y_train):\n",
    "        return FunctionSampler(func=SCIData.SMOTE, validate=False, kw_args=cls._static_params)\n",
    "\n",
    "class Resampler_RandomUnderSampler(Model):\n",
    "    _name = 'RandomUnderSampler'\n",
    "    _estimator = RandomUnderSampler\n",
    "\n",
    "    _static_params = dict(\n",
    "        random_state = 42,\n",
    "        replacement = False\n",
    "    )\n",
    "\n",
    "    _tuning_params_default = dict(\n",
    "        sampling_strategy = 0.1\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        return cls.compile_params(dict(\n",
    "            sampling_strategy = trial.suggest_float(\n",
    "                f'{cls._name}__sampling_strategy', 0.05, 0.5\n",
    "            )\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "@dataclass\n",
    "class Model_LightGBM(Model):\n",
    "    _name = 'LightGBM'\n",
    "    _estimator = LGBMClassifier\n",
    "\n",
    "    _requirements = dict(\n",
    "        onehot = False,\n",
    "        imputation = False,\n",
    "        fillna = False,\n",
    "        resampling = False\n",
    "    )\n",
    "\n",
    "    _static_params = dict(\n",
    "        objective='binary',\n",
    "        metric=['l2', 'auc'],\n",
    "        boosting_type='gbdt',\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "    )\n",
    "\n",
    "    _tuning_params_default = dict(\n",
    "        is_unbalance=True,\n",
    "        reg_alpha = 1.8e-3,\n",
    "        reg_lambda=6e-4,\n",
    "        num_leaves=14,\n",
    "        colsample_bytree=0.4,\n",
    "        subsample=0.97,\n",
    "        subsample_freq=1,\n",
    "        min_child_samples=6\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            reg_alpha = trial.suggest_float(\n",
    "                f'{cls._name}__reg_alpha', 1e-4, 10.0, log=True \n",
    "            ),\n",
    "            reg_lambda = trial.suggest_float(\n",
    "                f'{cls._name}__reg_lambda', 1e-4, 10.0, log=True \n",
    "            ),\n",
    "            num_leaves = trial.suggest_int(\n",
    "                f'{cls._name}__num_leaves', 2, 256\n",
    "            ),\n",
    "            colsample_bytree = trial.suggest_float(\n",
    "                f'{cls._name}__colsample_bytree', 0.4, 1.0\n",
    "            ),\n",
    "            subsample = trial.suggest_float(\n",
    "                f'{cls._name}__subsample', 0.4, 1.0\n",
    "            ),\n",
    "            subsample_freq = trial.suggest_int(\n",
    "                f'{cls._name}__subsample_freq', 1, 7\n",
    "            ),\n",
    "            min_child_samples = trial.suggest_int(\n",
    "                f'{cls._name}__min_child_samples', 5, 150\n",
    "            ),\n",
    "            is_unbalance = trial.suggest_categorical(\n",
    "                f'{cls._name}__is_unbalance', [True, False]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if not suggestions['is_unbalance']:\n",
    "            suggestions['scale_pos_weight'] = trial.suggest_int(\n",
    "                f'{cls._name}__scale_pos_weight', 1, 100\n",
    "            )\n",
    "\n",
    "        return cls.compile_params(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "class Model_XGBoost(Model):\n",
    "    _name = 'XGBoost'\n",
    "    _estimator = XGBClassifier\n",
    "\n",
    "    _requirements = dict(\n",
    "        onehot = False,\n",
    "        imputation = False,\n",
    "        fillna = False,\n",
    "        resampling = False\n",
    "    )\n",
    "\n",
    "    _static_params = dict(\n",
    "        verbosity = 0,\n",
    "        n_jobs = 1,\n",
    "        objective = 'binary:logistic',\n",
    "        booster = 'gbtree',\n",
    "        enable_categorical = True\n",
    "    )\n",
    "\n",
    "    _tuning_params_default = {\n",
    "        **dict(\n",
    "            tree_method=\"hist\",\n",
    "            alpha = 7e-05,\n",
    "            subsample = 0.42,\n",
    "            colsample_bytree = 0.87,\n",
    "            scale_pos_weight = 14,\n",
    "            max_depth = 7,\n",
    "            min_child_weight = 10,\n",
    "            eta = 0.035,\n",
    "            gamma = 4e-08,\n",
    "            grow_policy = 'lossguide'\n",
    "        ),\n",
    "        'lambda': 7e-2\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            tree_method=trial.suggest_categorical(\n",
    "                f\"{cls._name}__tree_method\", [\"approx\", \"hist\"]\n",
    "            ),\n",
    "            alpha=trial.suggest_float(f\"{cls._name}__alpha\", 1e-8, 1.0, log=True),\n",
    "            subsample=trial.suggest_float(f\"{cls._name}__subsample\", 0.2, 1.0),\n",
    "            colsample_bytree=trial.suggest_float(\n",
    "                f\"{cls._name}__colsample_bytree\", 0.2, 1.0\n",
    "            ),\n",
    "            scale_pos_weight=trial.suggest_int(f\"{cls._name}__scale_pos_weight\", 1, 100),\n",
    "            max_depth=trial.suggest_int(f\"{cls._name}__max_depth\", 3, 9, step=2),\n",
    "            min_child_weight=trial.suggest_int(\n",
    "                f\"{cls._name}__min_child_weight\", 2, 10\n",
    "            ),\n",
    "            eta=trial.suggest_float(f\"{cls._name}__eta\", 1e-8, 1.0, log=True),\n",
    "            gamma=trial.suggest_float(f\"{cls._name}__gamma\", 1e-8, 1.0, log=True),\n",
    "            grow_policy=trial.suggest_categorical(\n",
    "                f\"{cls._name}__grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "            )\n",
    "        )\n",
    "        suggestions[\"lambda\"] = trial.suggest_float(f\"{cls._name}__lambda\", 1e-8, 1.0, log=True)\n",
    "\n",
    "        return cls.compile_params(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class Model_LogisticRegression(Model):\n",
    "    _name = \"LogisticRegression\"\n",
    "    _estimator = LogisticRegression\n",
    "\n",
    "    _requirements = dict(\n",
    "        onehot = True,\n",
    "        imputation = True,\n",
    "        fillna = True,\n",
    "        resampling = False\n",
    "    )\n",
    "\n",
    "    _static_params = dict(\n",
    "        max_iter = 10000,\n",
    "        solver = 'saga',\n",
    "        random_state = 42,\n",
    "    )\n",
    "\n",
    "    _tuning_params_default = dict(\n",
    "        penalty = 'l2',\n",
    "        C = 5.9,\n",
    "        class_weight = 'balanced'\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def suggest_parameters(cls, trial):\n",
    "        suggestions = dict(\n",
    "            penalty = trial.suggest_categorical(\n",
    "                f'{cls._name}__penalty', ['l1', 'l2', 'elasticnet']\n",
    "            ),\n",
    "            C = trial.suggest_float(\n",
    "                f'{cls._name}__C', 0.01, 10\n",
    "            ),\n",
    "            class_weight = trial.suggest_categorical(\n",
    "                f'{cls._name}__class_weight', [None, 'balanced']\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if suggestions['penalty'] == 'elasticnet':\n",
    "            suggestions['l1_ratio'] = trial.suggest_float(\n",
    "                f'{cls._name}__l1_ratio', 0.05, 0.95\n",
    "            )\n",
    "\n",
    "        return cls.compile_params(suggestions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_studies():\n",
    "    study_grid = dict(\n",
    "        model = [Model_LogisticRegression, Model_LightGBM, Model_XGBoost],\n",
    "        resampler = [None, Resampler_RandomUnderSampler, Resampler_SMOTE],\n",
    "        impute = [True, False],\n",
    "    )\n",
    "\n",
    "    k, v = zip(*study_grid.items())\n",
    "    return [dict(zip(k, _)) for _ in itertools.product(*v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from typing import Dict, Any, Iterable, Optional\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def construct_study(model: Model, sci_train: SCIData, sci_test: SCIData, features: Iterable[str], impute: bool, resampler: Optional[Model]=None, cv=5, scoring='average_precision', storage=Notebook.TRIAL_STORAGE):\n",
    "    X_train, y_train = sci_train.xy(\n",
    "       imputation=impute, onehot_encoding = model._requirements['onehot'], fillna = model._requirements['fillna']\n",
    "    )\n",
    "\n",
    "    steps = [(model._name, model.get(X_train, y_train))]\n",
    "    if resampler is not None:\n",
    "        steps = [(resampler._name, resampler.get(X_train, y_train))] + steps\n",
    "    \n",
    "    pipeline_factory = lambda _: ImbPipeline(steps=steps).set_params(**_)\n",
    "    \n",
    "    objective = lambda trial: cross_validate(pipeline_factory({\n",
    "            **(resampler.suggest_parameters(trial) if resampler else {}),\n",
    "            **model.suggest_parameters(trial)\n",
    "        }), X_train, y_train, cv=cv, scoring=scoring, n_jobs=Notebook.CROSS_VAL_N_JOBS)['test_score'].mean()\n",
    "\n",
    "    name = f\"{model._name}:{resampler._name if resampler else 'None'}:{'Non' if not impute else ''}Imputed:{features}\"\n",
    "    study = optuna.create_study(direction='maximize', study_name=name, storage=storage)\n",
    "\n",
    "    return lambda **_: study.optimize(objective, **_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-30 17:50:26,820]\u001b[0m A new study created in RDB with name: LogisticRegression:None:Imputed:['features']\u001b[0m\n",
      "\u001b[32m[I 2022-10-30 17:50:28,180]\u001b[0m A new study created in RDB with name: LogisticRegression:None:NonImputed:['features']\u001b[0m\n",
      "\u001b[32m[I 2022-10-30 17:50:30,027]\u001b[0m A new study created in RDB with name: LogisticRegression:RandomUnderSampler:Imputed:['features']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "studies = [construct_study(**_, features=['features'], sci_train=sci_train, sci_test=sci_test) for _ in get_studies()]\n",
    "\n",
    "print('Starting execution..')\n",
    "with parallel_backend(\"loky\", inner_max_num_threads=5):\n",
    "    results = Parallel(n_jobs=5)(delayed(_)(n_trials=Notebook.N_TRIALS, timeout=Notebook.TRIALS_TIMEOUT, n_jobs=Notebook.N_PROCESSES) for _ in studies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e35166262197b1ea9223463adaf11f6b58d81a82b7650a41ad4f3574b9c5682"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
