{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import warnings, math, itertools, json, logging\n",
    "from hashlib import md5\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from dataset import SCIData, SCICols\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils.data_profiling, dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_theme(style=\"darkgrid\", palette=\"colorblind\")\n",
    "sns.set(rc={\"figure.figsize\": (11.5, 8.5), \"figure.dpi\": 100})\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Notebook:\n",
    "    DATASET_CACHE = {}\n",
    "    RUN_MODEL_SELECTION = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def sci_variant_cached(sci, **kwargs):\n",
    "    key = md5(json.dumps(kwargs, sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "    if key in Notebook.DATASET_CACHE:\n",
    "        return Notebook.DATASET_CACHE[key]\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    logging.info(f\"Generating SCI variant: {kwargs}\")\n",
    "    X, y = sci.preprocess_from_params(**kwargs)\n",
    "\n",
    "    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    Notebook.DATASET_CACHE[key] = (X_train, y_train)\n",
    "\n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_params = {\n",
    "    \"news_columns\": [\"impute\", \"omit\", \"mandate\"],\n",
    "    \"blood_columns\": [\"impute\", \"omit\", \"mandate\"],\n",
    "    \"news_format\": [\"raw\", \"scored\"],\n",
    "    \"diagnoses_grouping\": [\"icd10_3code\", \"icd10_group\", \"hsmr\", \"shmi\"],\n",
    "    \"diagnoses_onehot\": [False, True],\n",
    "    \"categorical_encoding\": [False, True],\n",
    "    \"numerical_scaling\": [False, True],\n",
    "    \"outcome\": ['DiedDuringStay', 'DiedWithin30Days', 'Readmitted', 'CriticalCare', 'CriticalEvent']\n",
    "    # Assumed steps:\n",
    "    # - Mandate diagnoses, omit a&e text, omit VBG\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_variants = {\n",
    "#     'No_missing_values': sci.mandate(SCICols.news_data_raw + SCICols.blood + SCICols.ae).augment_hsmr(onehot=True),\n",
    "#     'No_missing_vitals': sci.mandate(SCICols.news_data_raw + SCICols.blood).omit_ae().augment_hsmr(onehot=True),\n",
    "#     'News_only': SCIData(sci[SCICols.news_data_raw]),\n",
    "#     'Vitals_only': SCIData(sci[SCICols.news_data_raw + SCICols.blood]),\n",
    "#     'Vitals_and_age': SCIData(sci[SCICols.news_data_raw + SCICols.blood + SCICols.patient[:-2]]),\n",
    "#    # 'Vitals_and_ae': SCIData(sci[SCICols.news_data_raw + SCICols.blood + SCICols.patient[:-2] + SCICols.ae])\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def generate_sci(r, **kwargs):\n",
    "    for _ in [\"news\", \"blood\"]:\n",
    "        v = kwargs.get(f\"{_}_columns\")\n",
    "        if v is not None:\n",
    "            r = getattr(r, f\"{v}_{_}\")()\n",
    "\n",
    "    v = kwargs.get(\"news_format\")\n",
    "    if v == \"raw\":\n",
    "        r = r.raw_news()\n",
    "    elif v == \"scored\":\n",
    "        r = r.scored_news()\n",
    "\n",
    "    v, e = kwargs.get(\"diagnoses_grouping\"), kwargs.get(\"diagnoses_onehot\", False)\n",
    "    if v is not None:\n",
    "        r = getattr(r, f\"augment_{v}\")(onehot=e)\n",
    "\n",
    "    v = kwargs.get(\"numerical_scaling\", False)\n",
    "    numeric_pipeline = Pipeline(steps=[(\"scale\", MinMaxScaler())])\n",
    "\n",
    "    v = kwargs.get(\"categorical_encoding\", False)\n",
    "    categorical_pipeline = Pipeline(\n",
    "        steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))]\n",
    "    )\n",
    "\n",
    "    r = r.mandate_diagnoses()\n",
    "    return r.xy(outcome = kwargs.get('outcome', 'DiedDuringStay'))\n",
    "    #return transform_sci(X, **kwargs), y\n",
    "\n",
    "\n",
    "def transform_sci(X, **kwargs):\n",
    "    numeric_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"scale\",\n",
    "                MinMaxScaler() if kwargs.get(\"numeric_scaling\") else \"passthrough\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"one-hot\",\n",
    "                OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "                if kwargs.get(\"categorical_onehot\")\n",
    "                else \"passthrough\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ct = make_column_transformer(\n",
    "        (numeric_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "        (categorical_pipeline, make_column_selector(dtype_include=object)),\n",
    "    )\n",
    "\n",
    "    full_processor = Pipeline(steps=[(\"columns\", ct)])\n",
    "\n",
    "    return full_processor.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci = SCIData.load(\"data/sci_processed.h5\").omit_redundant().omit_vbg().omit_ae().derive_critical_event()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_params = {\n",
    "    \"news_columns\": \"impute\",\n",
    "    \"blood_columns\": \"impute\",\n",
    "    \"news_format\": \"raw\",\n",
    "    \"diagnoses_grouping\": \"hsmr\",\n",
    "    \"diagnoses_onehot\": True,\n",
    "    \"outcome\": \"DiedDuringStay\"\n",
    "}\n",
    "X, y = generate_sci(sci, **some_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2880"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(makeGrid(possible_params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    fbeta_score,\n",
    "    make_scorer,\n",
    ")\n",
    "\n",
    "f2_score = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "METRICS = {\n",
    "    \"Accuracy\": \"accuracy\",\n",
    "    \"Precision\": \"precision\",\n",
    "    \"Recall\": \"recall\",\n",
    "    \"AUC\": \"roc_auc\",\n",
    "    \"F1 Score\": \"f1\",\n",
    "    \"F2 Score\": f2_score,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeGrid(pars_dict):\n",
    "    keys = pars_dict.keys()\n",
    "    combinations = product(*pars_dict.values())\n",
    "    return (dict(zip(keys, cc)) for cc in combinations)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "def spotCheckCV(model, X, y, cv=3, pretty=True):\n",
    "    scores = cross_validate(model, X, y, scoring=METRICS, cv=cv)\n",
    "    if pretty:\n",
    "        display(\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    (name.split(\"_\")[1], sc)\n",
    "                    for name, score in scores.items()\n",
    "                    if name.startswith(\"test\")\n",
    "                    for sc in score\n",
    "                ],\n",
    "                columns=[\"Metric\", \"Score\"],\n",
    "            )\n",
    "            .groupby(\"Metric\")\n",
    "            .agg(\n",
    "                Mean=pd.NamedAgg(column=\"Score\", aggfunc=np.mean),\n",
    "                Std=pd.NamedAgg(column=\"Score\", aggfunc=np.std),\n",
    "            )\n",
    "        )\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Notebook.RUN_MODEL_SELECTION:\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "\n",
    "    naive_clf = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "    spotCheckCV(naive_clf, X_train, y_train, pretty=False), \"Naive Classifier\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "MODELS = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=0, C=1e2, max_iter=1000),\n",
    "    \"Gaussian NB\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "}\n",
    "\n",
    "IMBALANCED_MODELS = {\n",
    "    \"Balanced Decision Tree\": DecisionTreeClassifier(class_weight=\"balanced\"),\n",
    "    \"Balanced SVM\": SVC(gamma=\"scale\", class_weight=\"balanced\"),\n",
    "    \"Balanced Random Forest\": BalancedRandomForestClassifier(\n",
    "        n_estimators=10, class_weight=\"balanced_subsample\"\n",
    "    ),\n",
    "    \"Balanced XGBoost\": XGBClassifier(\n",
    "        use_label_encoder=False, scale_pos_weight=21, eval_metric=\"logloss\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "RESAMPLERS = {\n",
    "    \"SMOTE\": SMOTE(),\n",
    "    \"Undersampling\": RandomUnderSampler(sampling_strategy=\"majority\"),\n",
    "    \"SMOTE & Under\": ImbPipeline(\n",
    "        [\n",
    "            (\"oversampling\", SMOTE(sampling_strategy=0.1)),\n",
    "            (\"undersampling\", RandomUnderSampler(sampling_strategy=0.5)),\n",
    "        ]\n",
    "    ),\n",
    "    \"SMOTE & Tomek\": SMOTETomek(tomek=TomekLinks(sampling_strategy=\"majority\")),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Any, Dict\n",
    "from sklearn.base import BaseEstimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Task = NamedTuple(\n",
    "    \"Task\",\n",
    "    [\n",
    "        (\"name\", str),\n",
    "        (\"model\", BaseEstimator),\n",
    "        (\"preprocessor\", BaseEstimator),\n",
    "        (\"data_params\", Dict[str, Any]),\n",
    "        (\"model_params\", Dict[str, Any]),\n",
    "    ],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [\n",
    "    Task(\n",
    "        \"LR\",\n",
    "        LogisticRegression,\n",
    "        ct,\n",
    "        some_params,\n",
    "        {\"random_state\": 0, \"C\": 1e2, \"max_iter\": 1000},\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>0.833291</td>\n",
       "      <td>0.018894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.965149</td>\n",
       "      <td>0.001996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.170111</td>\n",
       "      <td>0.063223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2 Score</th>\n",
       "      <td>0.127861</td>\n",
       "      <td>0.050026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.387931</td>\n",
       "      <td>0.126991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.109756</td>\n",
       "      <td>0.043970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Mean       Std\n",
       "Metric                       \n",
       "AUC        0.833291  0.018894\n",
       "Accuracy   0.965149  0.001996\n",
       "F1 Score   0.170111  0.063223\n",
       "F2 Score   0.127861  0.050026\n",
       "Precision  0.387931  0.126991\n",
       "Recall     0.109756  0.043970"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in TASKS:\n",
    "    X, y = sci_variant_cached(SCIData(sci.head(10000)), **task.data_params)\n",
    "    clf = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", task.preprocessor),\n",
    "            (\"classifier\", task.model(**task.model_params)),\n",
    "        ]\n",
    "    )\n",
    "    spotCheckCV(clf, X, y, pretty=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e35166262197b1ea9223463adaf11f6b58d81a82b7650a41ad4f3574b9c5682"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
