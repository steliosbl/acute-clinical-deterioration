{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Preprocessing\n",
    "\n",
    "Cleans up and exports the original ACP dataset files into HDF5-compressed Pandas DataFrames.\n",
    "\n",
    "AEData:\n",
    " - Raw data must be unzipped into a directory.\n",
    " - Default path for the raw data is `data/AEdata`. This can be modified from the `Notebook` class.\n",
    "\n",
    "ICD10: \n",
    " - Original spreadsheet for ICD-10 (March 2021) can be found [here](https://www.health.gov.za/icd-10-master-industry-table/).\n",
    " - This should be placed in the `data/ICD10` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "\n",
    "from dataset import SCIData, SCICols\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Notebook:\n",
    "    DATA_DIR = Path(\"data\")\n",
    "    RAW_DIR = Path(\"data/AEdata\")\n",
    "\n",
    "    RUN_ALL = False\n",
    "\n",
    "    # Enable ONLY if running as a script\n",
    "    MULTITHREADING = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_per_dtype(df):\n",
    "    col_counts = {col: df[col].value_counts() for col in df.columns}\n",
    "    new_dict = defaultdict(list)\n",
    "    for k, v in df.dtypes.iteritems():\n",
    "        new_dict[v].append(k)\n",
    "    for k in new_dict.keys():\n",
    "        print(f\"{k}: {new_dict[k]}\")\n",
    "\n",
    "\n",
    "def col_counts_topn(df, n=10):\n",
    "    col_counts = {col: df[col].value_counts() for col in df.columns}\n",
    "    new_dict = defaultdict(list)\n",
    "    for k, v in col_counts.items():\n",
    "        new_dict[v.size].append(k)\n",
    "    for k in sorted(new_dict.keys())[:n]:\n",
    "        print(f\"{k}: {new_dict[k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Dates File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf5_correct_strings(df: pd.DataFrame):\n",
    "    string_cols = df.select_dtypes(include=\"object\").columns\n",
    "    df[string_cols] = df[string_cols].applymap(str)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_eddates(df: pd.DataFrame):\n",
    "    # Manual Correction: The first ArrivalDtm column has values in\n",
    "    # excel-style integer format (see https://stackoverflow.com/a/65460255/7662085)\n",
    "    if df.ArrivalDtm.dtype != np.dtype(\"datetime64[ns]\"):\n",
    "        df[\"ArrivalDtm\"] = pd.to_datetime(df.ArrivalDtm, unit=\"D\", origin=\"1899-12-30\")\n",
    "\n",
    "    # Iterate columns pairwise, and stack them vertically into a single DF\n",
    "    return pd.DataFrame(\n",
    "        np.concatenate(\n",
    "            [\n",
    "                df[list(_)].dropna().values\n",
    "                for _ in zip(df.columns[::3], df.columns[1::3])\n",
    "            ]\n",
    "        ),\n",
    "        columns=[\"AESerial\", \"ArrivalDtm\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_mainfiledates(df: pd.DataFrame):\n",
    "    return hdf5_correct_strings(df)\n",
    "\n",
    "\n",
    "def convert_event_dates(infile=\"Each event dateDONE.xlsx\", outfile=\"event_dates.h5\"):\n",
    "    result = {}\n",
    "\n",
    "    xlsx = pd.ExcelFile(Notebook.RAW_DIR / infile)\n",
    "    convert_sheets = {\n",
    "        \"EDDATES\": convert_eddates,\n",
    "        \"MainFileDates\": convert_mainfiledates,\n",
    "    }\n",
    "\n",
    "    for sheet, converter in convert_sheets.items():\n",
    "        result[sheet] = converter(pd.read_excel(xlsx, sheet, index_col=None))\n",
    "\n",
    "    with pd.HDFStore(Notebook.DATA_DIR / outfile) as store:\n",
    "        for name, df in result.items():\n",
    "            store[name] = df\n",
    "\n",
    "    return result.values()\n",
    "\n",
    "\n",
    "if Notebook.RUN_ALL:\n",
    "    convert_event_dates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCI File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_SCI(xlsx):\n",
    "    df = xlsx.copy()\n",
    "\n",
    "    # Remove spaces from column names\n",
    "    df.columns = df.columns.str.replace(\" \", \"\")\n",
    "\n",
    "    # Drop redundant columns\n",
    "    df = df.drop(SCICols.redundant, axis=1)\n",
    "\n",
    "    col_counts = {col: df[col].value_counts() for col in df.columns}\n",
    "\n",
    "    # Drop c_ prefixed columns with no values in them\n",
    "    df = df.drop(\n",
    "        [\n",
    "            col\n",
    "            for col, count in col_counts.items()\n",
    "            if count.size <= 2 and col.startswith(\"c_\")\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Replace NoNSw2d with np.NAN in all applicable columns\n",
    "    df = df.replace({\"NoNSw2d\": np.NAN, \"NoObW2D\": np.NAN, \"Noobw2d\": np.NAN})\n",
    "\n",
    "    df['c_Breathing_device'] = df.c_O2_device_or_air.copy()\n",
    "    \n",
    "    # Turn these 2-value string columns into binary\n",
    "    binarise = {\n",
    "        \"AdmissionType\": \"Elective\",\n",
    "        \"AdmissionArea\": \"Medical Assessment Area\",\n",
    "        \"DischargeArea\": \"Assessment Area Discharge\",\n",
    "        \"c_Nausea\": \"1 - Nausea present\",\n",
    "        \"c_Vomiting\": \"1 - Vomiting since last round\",\n",
    "        \"Gender\": \"Female\",\n",
    "        \"c_O2_device_or_air\": \"A - Air\",\n",
    "        \"c_Patient_Position\": \"Lying\",\n",
    "        \"c_Level_of_consciousness\": \"A - Alert\",\n",
    "        **{\n",
    "            _: \"Yes\"\n",
    "            for _ in [\n",
    "                \"Over7Days\",\n",
    "                \"Over14Days\",\n",
    "                \"CareHome\",\n",
    "                \"DiedDuringStay\",\n",
    "                \"DiedWithin30Days\",\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for col, true in binarise.items():\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .apply(true.__eq__)\n",
    "            .apply(lambda x: np.nan if x == NotImplemented else x)\n",
    "        )\n",
    "\n",
    "    df[\"c_O2_device_or_air\"] = df[\"c_O2_device_or_air\"].replace(\n",
    "        {True: False, False: True}\n",
    "    )\n",
    "\n",
    "    # Convert CFS dates\n",
    "    df.Noofminsafteradmission = df.AdmissionDateTime + pd.to_timedelta(\n",
    "        df.Noofminsafteradmission, unit=\"m\"\n",
    "    )\n",
    "    df.NoMinsBeforeadmission = df.AdmissionDateTime + pd.to_timedelta(\n",
    "        df.NoMinsBeforeadmission, unit=\"m\"\n",
    "    )\n",
    "\n",
    "    # Rename some of the binarised columns for better clarity\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"AdmissionType\": \"AdmittedAfterAEC\",\n",
    "            \"AdmissionArea\": \"AssessmentAreaAdmission\",\n",
    "            \"DischargeArea\": \"AssessmentAreaDischarge\",\n",
    "            \"c_Vomiting\": \"VomitingSinceLastRound\",\n",
    "            \"SpellDischargeDate\": \"DischargeDateTime\",\n",
    "            \"Gender\": \"Female\",\n",
    "            \"DischargeDestinationDescription\": \"DischargeDestination\",\n",
    "            \"c_O2_device_or_air\": \"c_AssistedBreathing\",\n",
    "            \"c_Patient_Position\": \"c_LyingDown\",\n",
    "            \"c_Breathing_device\": 'c_BreathingDevice',\n",
    "            \"c_Level_of_consciousness\": \"AVCPU_Alert\",\n",
    "            \"c_Heart_rate\":'HeartRate',\n",
    "            'c_BP_Diastolic': 'DiastolicBP',\n",
    "            'c_BP_Systolic': 'SystolicBP',\n",
    "            \"Noofminsafteradmission\": \"CFSAfterAdmissionDate\",\n",
    "            \"NoMinsBeforeadmission\": \"CFSBeforeAdmissionDate\",\n",
    "            \"CFSBeforeadmission\": \"CFSBeforeAdmission\",\n",
    "            'AdmissionWardLOS': 'AdmitWardLOS',\n",
    "            'NextWardLOS2': 'NextWard2LOS',\n",
    "            'NextWardLOS3': 'NextWard3LOS',\n",
    "            'NextWardLOS4': 'NextWard4LOS',\n",
    "            'NextWardLOS5': 'NextWard5LOS',\n",
    "            'NextWardLOS6': 'NextWard6LOS',\n",
    "            'NextWardLOS7': 'NextWard7LOS',\n",
    "            'NextWardLOS8': 'NextWard8LOS',\n",
    "            'NextWardLOS9': 'NextWard9LOS',\n",
    "            'DischargeWardLOS': 'DischargeWardLOS',\n",
    "            \"Urea(serum)\" : \"Urea_serum\",\n",
    "            \"Sodium(serum)\" : \"Sodium_serum\",\n",
    "            \"Potassium(serum)\" : \"Potassium_serum\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df.columns = [_[2:] if _ .startswith('c_') else _ for _ in df.columns]\n",
    "\n",
    "    df.Pain = df.Pain.map(\n",
    "        {\n",
    "            \"0 - No pain\": 0,\n",
    "            \"1 - Mild pain\": 1,\n",
    "            \"2 - Moderate pain\": 2,\n",
    "            \"3 - Severe pain\": 3,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Convert NEWS dates\n",
    "    datetimes = [\"NewsCreatedWhen\", \"NewsTouchedWhen\", \"NewsAuthoredDtm\"]\n",
    "    df[datetimes] = df[datetimes].apply(pd.to_datetime, errors=\"coerce\")\n",
    "\n",
    "    # Convert blood results\n",
    "    # Ignore certain non-numeric entries as they make up less than 0.001%\n",
    "    numeric = [\n",
    "        \"Urea_serum\",\n",
    "        \"Sodium_serum\",\n",
    "        \"Potassium_serum\",\n",
    "        \"Creatinine\",\n",
    "        \"pO2(POC)Venous\",\n",
    "    ]\n",
    "    df[numeric] = df[numeric].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Drop duplicates based on serial code\n",
    "    df = df.sort_values(\"SEQ\", ascending=False).drop_duplicates(\"SpellSerial\")\n",
    "\n",
    "    df = df.replace(\"nan\", np.nan)\n",
    "\n",
    "    df = df[df.AdmissionDateTime >= pd.Timestamp('2015-01-01')]\n",
    "\n",
    "    df = df[~df.AdmissionMethodDescription.isin(['TRAUMA ELECTIVE ADM', 'MATERNITY ANTE NATAL', 'WAITING LIST'])]\n",
    "\n",
    "    return df[SCICols.ordered()].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 12:56:34,044 [INFO] Reading file: data\\AEdata\\Copy of SCI11868 Delivered 7 Ian Browne.xlsx\n"
     ]
    }
   ],
   "source": [
    "if Notebook.RUN_ALL or True:\n",
    "    infile = Notebook.RAW_DIR / \"Copy of SCI11868 Delivered 7 Ian Browne.xlsx\"\n",
    "    outfile = Notebook.DATA_DIR / \"sci.h5\"\n",
    "\n",
    "    logging.info(f\"Reading file: {infile}\")\n",
    "    xlsx = pd.read_excel(infile)\n",
    "\n",
    "    logging.info(f\"Processing file: {infile}\")\n",
    "    df = process_SCI(xlsx)\n",
    "\n",
    "    logging.info(f\"Writing to: {outfile}/table\")\n",
    "    df.to_hdf(outfile, key=\"table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admissions Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_datetime(df, old, new):\n",
    "    df[new] = pd.to_datetime(\n",
    "        df[old]\n",
    "        .dropna(how=\"any\")\n",
    "        .astype(str)\n",
    "        .replace(\"\\.0\", \"\", regex=True)\n",
    "        .apply(\" \".join, 1),\n",
    "        format=\"%Y %m %W %A %H\",\n",
    "    )\n",
    "\n",
    "\n",
    "def process_AD(xlsx):\n",
    "    df = xlsx.copy()\n",
    "\n",
    "    # Construct DateTime from the individual columns describing admission/discharge date\n",
    "    adm_dt, disch_dt = (\n",
    "        [\"YearAdmit\", \"MonthAdmit\", \"AdmitWeek\", \"AdmitDay\", \"AdmitHour\"],\n",
    "        [\"YearDisch\", \"MonthDisch\", \"DischWeek\", \"DischDay\", \"DischHour\"],\n",
    "    )\n",
    "    reconstruct_datetime(df, adm_dt, \"AdmissionDateTime\")\n",
    "    if \"SpellDischargeDate\" in df.columns:\n",
    "        df = df.rename(columns={\"SpellDischargeDate\": \"DischargeDateTime\"})\n",
    "    else:\n",
    "        reconstruct_datetime(df, disch_dt, \"DischargeDateTime\")\n",
    "\n",
    "    # Turn these 2-value string columns into binary\n",
    "    binarise = {\n",
    "        \"AdmissionType\": \"Elective\",\n",
    "        \"AdmissionArea\": \"Medical Assessment Area\",\n",
    "        \"DischargeArea\": \"Assessment Area Discharge\",\n",
    "        **{\n",
    "            _: \"Yes\"\n",
    "            for _ in [\n",
    "                \"Over7Days\",\n",
    "                \"Over14Days\",\n",
    "                \"CareHome\",\n",
    "                \"DiedDuringStay\",\n",
    "                \"DiedWithin30Days\",\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for col, true in binarise.items():\n",
    "        df[col] = df[col].apply(true.__eq__).apply(lambda x: np.nan if x == NotImplemented else x)\n",
    "\n",
    "    # Rename some of the binarised columns for better clarity\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"AdmissionType\": \"ElectiveAdmission\",\n",
    "            \"AdmissionArea\": \"AssessmentAreaAdmission\",\n",
    "            \"DischargeArea\": \"AssessmentAreaDischarge\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Drop individual date component columns and some extraneous ones\n",
    "    df = df.drop(\n",
    "        adm_dt\n",
    "        + disch_dt\n",
    "        + [\n",
    "            \"DischargeFYear\",\n",
    "            \"AdmissionFYear\",\n",
    "            \"AdmissionFYMonth\",\n",
    "            \"AdmissionConsultant\",\n",
    "            \"LastConsultant\",\n",
    "            \"Area\",\n",
    "            \"PCT\",\n",
    "            \"GPPractice\",\n",
    "            \"AdmissionWardEndDate\",\n",
    "        ],\n",
    "        axis=1,\n",
    "        errors=\"ignore\",\n",
    "    )\n",
    "\n",
    "    # Drop duplicates based on serial code\n",
    "    df = df.drop_duplicates(\"SpellSerial\")\n",
    "\n",
    "    return df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_AD_single(infile):\n",
    "    logging.info(f\"Reading file: {infile}\")\n",
    "    xlsx = pd.read_excel(infile)\n",
    "\n",
    "    logging.info(f\"Processing file: {infile}\")\n",
    "    return process_AD(xlsx)\n",
    "\n",
    "\n",
    "if Notebook.RUN_ALL:\n",
    "    indir = \"AD\"\n",
    "    outfile = Notebook.DATA_DIR / \"AD.h5\"\n",
    "    results = []\n",
    "\n",
    "    if Notebook.MULTITHREADING:\n",
    "        with Pool(3) as p:\n",
    "            results = p.map(\n",
    "                process_AD_single, list((Notebook.RAW_DIR / indir).iterdir())\n",
    "            )\n",
    "    else:\n",
    "        for infile in (Notebook.RAW_DIR / indir).iterdir():\n",
    "            results.append(process_AD_single(infile))\n",
    "\n",
    "    logging.info(f\"Writing all to {outfile}\")\n",
    "    r = pd.concat(results)\n",
    "    r.to_hdf(outfile, key=\"table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICD-10 File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_icd10(xlsx):\n",
    "    ICD10_3_Codes = (\n",
    "        xlsx[\n",
    "            [\n",
    "                \"Chapter_No\",\n",
    "                \"Chapter_Desc\",\n",
    "                \"Group_Code\",\n",
    "                \"Group_Desc\",\n",
    "                \"ICD10_3_Code\",\n",
    "                \"ICD10_3_Code_Desc\",\n",
    "            ]\n",
    "        ]\n",
    "        .drop_duplicates(\"ICD10_3_Code\")\n",
    "        .set_index(\"ICD10_3_Code\")\n",
    "    )\n",
    "    ICD10_Codes = xlsx[[\"ICD10_Code\", \"ICD10_3_Code\", \"WHO_Full_Desc\"]].set_index(\n",
    "        \"ICD10_Code\"\n",
    "    )\n",
    "\n",
    "    return ICD10_3_Codes, ICD10_Codes\n",
    "\n",
    "\n",
    "if Notebook.RUN_ALL:\n",
    "    infile = Notebook.DATA_DIR / \"ICD10/ICD-10_MIT_2021_Excel_16-March_2021.xlsx\"\n",
    "    outfile = Notebook.DATA_DIR / \"icd10.h5\"\n",
    "\n",
    "    logging.info(f\"Reading file: {infile}\")\n",
    "    xlsx = pd.read_excel(infile, sheet_name=\"SA ICD-10 MIT 2021\")\n",
    "\n",
    "    logging.info(f\"Processing file: {infile}\")\n",
    "    tc, c = process_icd10(xlsx)\n",
    "\n",
    "    logging.info(f\"Writing to: {outfile}\")\n",
    "    with pd.HDFStore(outfile) as store:\n",
    "        store[\"ICD10_3_Codes\"], store[\"ICD10_Codes\"] = tc, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Notebook.RUN_ALL:\n",
    "    infile = Notebook.DATA_DIR / \"ICD10/Birkmeyer.csv\"\n",
    "    outfile = Notebook.DATA_DIR / \"birkmeyer_icd10.h5\"\n",
    "    logging.info(f'Reading file: {infile}')\n",
    "    df = pd.read_csv(infile)\n",
    "\n",
    "    logging.info(f\"Processing file: {infile}\")\n",
    "    #df = process_birkmeyer(df)\n",
    "\n",
    "    logging.info(f\"Writing to: {outfile}\")\n",
    "    df.to_hdf(outfile, 'table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_CCS(xlsx):\n",
    "    df = (\n",
    "        xlsx[xlsx[\"Inpatient Default CCSR (Y/N/X)\"].eq(\"Y\")]\n",
    "        .copy()\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"ICD-10-CM Code\": \"ICD10Code\",\n",
    "                \"CCSR Category\": \"CCSCode\",\n",
    "                \"CCSR Category Description\": \"CCSDescription\",\n",
    "                \"Rationale\": \"CCSRationale\",\n",
    "            }\n",
    "        )\n",
    "        .drop(\n",
    "            [\n",
    "                \"ICD-10-CM Code Description\",\n",
    "                \"Inpatient Default CCSR (Y/N/X)\",\n",
    "                \"Outpatient Default CCSR (Y/N/X)\",\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add dots to ICD-10 codes\n",
    "    # mask = df.ICD10Code.str.len() > 3\n",
    "    # df.loc[mask, \"ICD10Code\"] = (\n",
    "    #     df.loc[mask, \"ICD10Code\"].str[:3] + \".\" + df.loc[mask, \"ICD10Code\"].str[3:]\n",
    "    # )\n",
    "    df = df.set_index('ICD10Code')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_CCS2():\n",
    "    infile = 'data/ICD10/Classification of variables HSMR 2018.xlsx'\n",
    "    codes, aggregate = pd.read_excel(infile, 'ICD10'), pd.read_excel(infile, 'Aggregate'), \n",
    "    shmi = pd.read_excel('data/ICD10/SHMI diagnosis group breakdown, Mar21-Feb22.xlsx', \"Diagnosis group descriptions\")\n",
    "    \n",
    "    codes.columns = ['ICD10Code', 'CCSGroup', 'CCSGroupDescription']\n",
    "    aggregate.columns = ['CCSGroup', 'CCSGroupDescription', 'DiagnosisGroup', 'DiagnosisGroupDescription', 'AggregateGroup', 'AggregateGroupDescription']\n",
    "    shmi.columns = ['SHMIGroup', 'CCSGroup', 'CCSGroupDescription', 'SHMIGroupDescription']\n",
    "\n",
    "    codes, aggregate = codes.set_index('ICD10Code'), aggregate.set_index('CCSGroup')\n",
    "\n",
    "    shmi.CCSGroup = shmi.CCSGroup.str.split(', ')\n",
    "    shmi = shmi.explode('CCSGroup').set_index('CCSGroup')\n",
    "    shmi.index = pd.to_numeric(shmi.index)\n",
    "    shmi.SHMIGroup = pd.to_numeric(shmi.SHMIGroup)\n",
    "\n",
    "    return codes, aggregate.drop(['DiagnosisGroup', 'DiagnosisGroupDescription', 'CCSGroupDescription'], axis=1), shmi.sort_index().drop('CCSGroupDescription', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Notebook.RUN_ALL:\n",
    "    codes, aggregate, shmi = process_CCS2()\n",
    "    with pd.HDFStore(Notebook.DATA_DIR / 'ccs.h5') as store:\n",
    "        store['codes'] = codes\n",
    "        store['shmi'] = shmi\n",
    "        store['hsmr'] = aggregate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Notebook.RUN_ALL:\n",
    "    infile = Notebook.DATA_DIR / \"ICD10/DXCCSR-Reference-File-v2022-1.xlsx\"\n",
    "    outfile = Notebook.DATA_DIR / \"ccs_icd10.h5\"\n",
    "    xlsx = pd.read_excel(infile, \"DX_to_CCSR_Mapping\")\n",
    "\n",
    "    df = process_CCS(xlsx)\n",
    "\n",
    "    df.to_hdf(outfile, 'table')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e35166262197b1ea9223463adaf11f6b58d81a82b7650a41ad4f3574b9c5682"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
