{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACP Project - Predicting Critical Events "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 300)\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import f2_score, METRICS, evaluate, evaluate_from_pred, with_sampling_strategies, spotCheckCV, spotCheckDatasets, F2TabNet\n",
    "from utils.isolation_forest_wrapper import IsolationForestWrapper\n",
    "%aimport utils.evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Notebook:\n",
    "    IMAGE_DIR = 'images'\n",
    "    OUTCOME = \"CriticalEvent\"\n",
    "    MODELS = {}\n",
    "    EXPLAINERS = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of results (*pre-tuning*)\n",
    "\n",
    "See following sections for detailed results on each model, and the improvement achieved after tuning model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"old_code/ml_test_4/results.csv\").drop(\"dataset\", axis=1).set_index(\n",
    "    \"model\"\n",
    ").sort_values(\"test_F2 Score\", ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding/Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def get_threshold(y_train, y_pred_proba, target=0.85):\n",
    "    \"\"\" Given prediction probabilities, sets the prediction threshold to approach the given target recall\n",
    "    \"\"\"\n",
    "\n",
    "    # Get candidate thresholds from the model, and find the one that gives the best fbeta score\n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "        y_train, y_pred_proba\n",
    "    )\n",
    "    closest = thresholds[np.abs(recall-target).argmin()]\n",
    "    \n",
    "    return closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Different variations on this initial, pre-processed version will be tested throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SCIData, SCICols\n",
    "%aimport dataset\n",
    "\n",
    "# SCIData.load('data/sci.h5').clean_all().filter_vague_diagnoses().derive_readmission().omit_vbg()\n",
    "sci = SCIData.load('data/sci_processed.h5').fix_readmissionband().derive_critical_event(within=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scii = (\n",
    "    sci.omit_redundant()\n",
    "    .drop([\"ReadmissionBand\", \"AgeBand\"], axis=1)\n",
    "    .omit_ae()\n",
    "    .raw_news()\n",
    "    .mandate_news()\n",
    "    .mandate_blood()\n",
    "    .augment_hsmr()\n",
    ")\n",
    "\n",
    "sci_train, sci_test = train_test_split(\n",
    "    scii, test_size=0.33, random_state=42, stratify=scii[Notebook.OUTCOME]\n",
    ")\n",
    "sci_train, sci_test = SCIData(sci_train), SCIData(sci_test)\n",
    "\n",
    "# Drop HSMR_15 as there is only 1 in the entire dataset, making a split impossible\n",
    "(X_train, y_train), (X_test, y_test) = (\n",
    "    sci_train.encode_ccs_onehot()\n",
    "    .xy(outcome=Notebook.OUTCOME, ordinal_encoding=True, dropna=False, fillna=True),\n",
    "    sci_test.encode_ccs_onehot()\n",
    "    .xy(outcome=Notebook.OUTCOME, ordinal_encoding=True, dropna=False, fillna=True),\n",
    ")\n",
    "\n",
    "def drop_exclusive_cols(X1, X2):\n",
    "    exclusive_cols = set(X1.columns) ^ set(X2.columns)\n",
    "    X1.drop(exclusive_cols, axis=1, errors='ignore', inplace=True)\n",
    "    X2.drop(exclusive_cols, axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "drop_exclusive_cols(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scii = (\n",
    "    sci.omit_redundant()\n",
    "    .drop([\"ReadmissionBand\", \"AgeBand\"], axis=1)\n",
    "    .omit_ae()\n",
    "    .raw_news()\n",
    ")\n",
    "\n",
    "datasets = {\n",
    "    \"Mandated vitals, One-hot diagnoses\": (\n",
    "        scii.mandate_news()\n",
    "        .mandate_blood()\n",
    "        .augment_hsmr()\n",
    "        .encode_ccs_onehot()\n",
    "        .xy(outcome=Notebook.OUTCOME, ordinal_encoding=True, dropna=True)\n",
    "    ),\n",
    "    \"Mandated vitals, Categorical diagnoses (main only)\": (\n",
    "        scii.mandate_news()\n",
    "        .mandate_blood()\n",
    "        .augment_hsmr()\n",
    "        .drop(SCICols.diagnoses[1:], axis=1)\n",
    "        .xy(outcome=Notebook.OUTCOME, ordinal_encoding=True, dropna=True)\n",
    "    ),\n",
    "    \"Mandated vitals, Categorical diagnoses (with missing)\": (\n",
    "        scii.mandate_news()\n",
    "        .mandate_blood()\n",
    "        .augment_hsmr()\n",
    "        .drop(SCICols.diagnoses[1:], axis=1)\n",
    "        .xy(outcome=Notebook.OUTCOME, ordinal_encoding=True, fillna=True)\n",
    "    ),\n",
    "    \"Imputed vitals\": (\n",
    "        scii.impute_news()\n",
    "        .impute_blood()\n",
    "        .augment_hsmr()\n",
    "        .encode_ccs_onehot()\n",
    "        .xy(outcome=Notebook.OUTCOME, ordinal_encoding=True, dropna=True)\n",
    "    ),\n",
    "    \"Missing NEWS, imputed blood\": (\n",
    "        scii.augment_hsmr()\n",
    "        .impute_blood()\n",
    "        .encode_ccs_onehot()\n",
    "        .mandate_diagnoses()\n",
    "        .xy(outcome=Notebook.OUTCOME, ordinal_encoding=True, fillna=True)\n",
    "    ),\n",
    "    \"Missing vitals\": (\n",
    "        scii.augment_hsmr()\n",
    "        .impute_blood()\n",
    "        .encode_ccs_onehot()\n",
    "        .mandate_diagnoses()\n",
    "        .xy(outcome=Notebook.OUTCOME, ordinal_encoding=True, fillna=True)\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEWS only\n",
    "Logistic regression (L2 penalty, by default) slightly beats the baseline NEWS model, which had AUC `0.807` for this outcome, but doesn't beat the original study score of `0.873`.\n",
    "\n",
    "Balanced variant completely flips the precision vs. recall relationship, so it does a better job of minimising false-negatives. However, the final values are still low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "datasets_lr = {\n",
    "    \"Raw NEWS\": sci.mandate(SCICols.news_data_raw).xy(\n",
    "        outcome=Notebook.OUTCOME, x=SCICols.news_data_raw, dtype=float\n",
    "    ),\n",
    "    \"Scored NEWS\": sci.mandate(SCICols.news_data_scored).xy(\n",
    "        outcome=Notebook.OUTCOME, x=SCICols.news_data_scored, dtype=float\n",
    "    ),\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"LR Balanced\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=1000, random_state=42\n",
    "    ),\n",
    "}\n",
    "\n",
    "spotCheckDatasets(models, datasets_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotCheckCV(\n",
    "    {\n",
    "        \"LR\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        **with_sampling_strategies(\n",
    "            LogisticRegression(max_iter=1000, random_state=42), \"LR\"\n",
    "        ),\n",
    "    },\n",
    "    *datasets_lr[\"Raw NEWS\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(\n",
    "    *sci.mandate(SCICols.news_data_raw).xy(\n",
    "        outcome=Notebook.OUTCOME, x=SCICols.news_data_raw, dtype=float\n",
    "    ),\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "evaluate(\n",
    "    LogisticRegression(class_weight=\"balanced\", max_iter=1000, random_state=42).fit(\n",
    "        X_train_news, y_train_news\n",
    "    ),\n",
    "    X_test_news,\n",
    "    y_test_news,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding\n",
    "\n",
    "The baseline model's pre-defined risk ratio (NEWS score $\\geq 7$) yields its own precision and recall \"ratio\". I.e., it balances false-positives and negatives in a certain way. We can emulate this tradeoff by adjusting the class threshold in the regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score\n",
    "\n",
    "\n",
    "def get_news_target_ratio(X, y):\n",
    "    y_pred = X.c_NEWS_score >= 7\n",
    "    p = precision_score(y, y_pred)\n",
    "    r = recall_score(y, y_pred)\n",
    "    print(f\"NEWS Precision={p:.5f}, Recall={r:.5f}\")\n",
    "    return p / r\n",
    "\n",
    "\n",
    "# Get the \"target\" ratio of precision/recall that the NEWS threshold produces\n",
    "target_ratio = get_news_target_ratio(\n",
    "    *sci.mandate(SCICols.news_data_raw).xy(outcome=Notebook.OUTCOME)\n",
    ")\n",
    "\n",
    "# Train the LR model\n",
    "model = LogisticRegression(class_weight=\"balanced\", random_state=42, max_iter=1000).fit(\n",
    "    X_train_news, y_train_news\n",
    ")\n",
    "\n",
    "# Get candidate thresholds from the model, and find the one that gives the ratio closest to the target\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_train_news, model.predict_proba(X_train_news)[:, 1]\n",
    ")\n",
    "closest = thresholds[\n",
    "    np.abs(\n",
    "        np.divide(precision, recall, out=np.ones_like(recall), where=recall != 0)\n",
    "        - target_ratio\n",
    "    ).argmin()\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test set, this gives much improved F2, while the AUC remains consistent (slightly better than baseline model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions on the test set, using this new threshold\n",
    "y_pred_proba = model.predict_proba(X_test_news)\n",
    "y_pred = np.where(y_pred_proba[:, 1] > closest, 1, 0)\n",
    "\n",
    "# Produce scores\n",
    "evaluate_from_pred(y_test_news, y_pred, y_pred_proba[:, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Logistic Regression (balanced)\": LogisticRegression(\n",
    "        max_iter=1000, class_weight=\"balanced\"\n",
    "    ),\n",
    "    \"Logistic Regression (saga)\": Pipeline(\n",
    "        steps=[\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\n",
    "                \"clf\",\n",
    "                LogisticRegression(\n",
    "                    max_iter=1000,\n",
    "                    class_weight=\"balanced\",\n",
    "                    solver=\"saga\",\n",
    "                    penalty=\"elasticnet\",\n",
    "                    l1_ratio=0.5,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "spotCheckDatasets(models, datasets_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[\"Mandated vitals, One-hot diagnoses\"]\n",
    "categorical_cols_idx = SCIData(dataset[0]).describe_categories()[0]\n",
    "spotCheckCV(\n",
    "    {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Logistic Regression (balanced)\": LogisticRegression(\n",
    "            max_iter=1000, class_weight=\"balanced\"\n",
    "        ),\n",
    "        **with_sampling_strategies(\n",
    "            LogisticRegression(max_iter=1000),\n",
    "            \"Logistic Regression\",\n",
    "            categorical_cols_idx,\n",
    "        ),\n",
    "        **with_sampling_strategies(\n",
    "            LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
    "            \"Logistic Regression (balanced)\",\n",
    "            categorical_cols_idx,\n",
    "        ),\n",
    "    },\n",
    "    *dataset\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, class_weight=\"balanced\").fit(X_train, y_train)\n",
    "\n",
    "evaluate(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    plot_title=\"Logistic Regression (non-tuned)\",\n",
    "    save=f\"{Notebook.IMAGE_DIR}/eval_logistic_regression.png\",\n",
    ")\n",
    "\n",
    "Notebook.MODELS[\"Logistic Regression\"] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Notebook.MODELS[\"Logistic Regression\"]\n",
    "\n",
    "threshold = get_threshold(\n",
    "    y_train, \n",
    "    model.predict_proba(X_train)[:, 1], \n",
    ")\n",
    "\n",
    "# Create predictions on the test set, using this new threshold\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = np.where(y_pred_proba > threshold, 1, 0)\n",
    "\n",
    "# Produce scores\n",
    "evaluate_from_pred(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    y_pred_proba,\n",
    "    plot_title=\"Logistic Regression (tuned)\",\n",
    "    save=f\"{Notebook.IMAGE_DIR}/eval_thresholded_logistic_regression.png\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.LinearExplainer(model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "Notebook.EXPLAINERS[\"Logistic Regression\"] = (explainer, shap_values)\n",
    "\n",
    "fig = shap.plots.beeswarm(shap_values, max_display=30, show=False)\n",
    "plt.title(\"Logistic Regression\")\n",
    "plt.savefig(f\"{Notebook.IMAGE_DIR}/shap_swarm_logistic_regression.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_class_idx = np.where(model.predict(X_test))[0][-1]\n",
    "fig = shap.plots.force(shap_values[positive_class_idx])\n",
    "\n",
    "shap.save_html(f\"{Notebook.IMAGE_DIR}/force_plot_logistic_regression.html\", fig)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Achieves best-in-class performance (nearly matched by LightGBM). Performs best on the dataset variant with mandated vitals data and one-hot encoded diagnoses with random undersampling to combat the class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_xgb = {\n",
    "    \"Mandated vitals, One-hot diagnoses\": (\n",
    "        scii.mandate_news()\n",
    "        .mandate_blood()\n",
    "        .augment_hsmr()\n",
    "        .encode_ccs_onehot()\n",
    "        .xy(outcome=Notebook.OUTCOME, dropna=True)\n",
    "    ),\n",
    "    \"Mandated vitals, Categorical diagnoses\": (\n",
    "        scii.mandate_news()\n",
    "        .mandate_blood()\n",
    "        .impute_blood()\n",
    "        .augment_hsmr()\n",
    "        .xy(outcome=Notebook.OUTCOME)\n",
    "    ),\n",
    "    \"Imputed vitals, One-hot diagnoses\": (\n",
    "        scii.impute_news()\n",
    "        .impute_blood()\n",
    "        .augment_hsmr()\n",
    "        .encode_ccs_onehot()\n",
    "        .xy(outcome=Notebook.OUTCOME)\n",
    "    ),\n",
    "    \"Mandated NEWS, imputed blood, One-hot diagnoses\": (\n",
    "        scii.mandate_news()\n",
    "        .impute_blood()\n",
    "        .augment_hsmr()\n",
    "        .encode_ccs_onehot()\n",
    "        .xy(outcome=Notebook.OUTCOME)\n",
    "    ),\n",
    "    \"Missing NEWS, imputed blood, One-hot diagnoses\": (\n",
    "        scii.augment_hsmr()\n",
    "        .impute_blood()\n",
    "        .encode_ccs_onehot()\n",
    "        .xy(outcome=Notebook.OUTCOME)\n",
    "    ),\n",
    "    \"Missing blood & NEWS, One-hot diagnoses\": (\n",
    "        scii.augment_hsmr()\n",
    "        .impute_blood()\n",
    "        .encode_ccs_onehot()\n",
    "        .xy(outcome=Notebook.OUTCOME)\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_weight_lambda = lambda X, y: dict(scale_pos_weight=y.shape[0] / y.sum())\n",
    "\n",
    "xgb_set_params = {\n",
    "    \"XGB Balanced\": xgb_weight_lambda,\n",
    "    \"Hist XGB Balanced\": xgb_weight_lambda,\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Approx XGB\": XGBClassifier(\n",
    "        tree_method=\"approx\", enable_categorical=True, scale_pos_weight=1\n",
    "    ),\n",
    "    \"Approx XGB Balanced\": XGBClassifier(\n",
    "        tree_method=\"approx\", enable_categorical=True,\n",
    "    ),\n",
    "    \"Hist XGB\": XGBClassifier(\n",
    "        tree_method=\"hist\", enable_categorical=True, scale_pos_weight=1\n",
    "    ),\n",
    "    \"Hist XGB Balanced\": XGBClassifier(tree_method=\"hist\", enable_categorical=True,),\n",
    "}\n",
    "\n",
    "spotCheckDatasets(datasets=datasets_xgb, models=models, set_params=xgb_set_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets_xgb[\"Mandated NEWS, imputed blood, One-hot diagnoses\"]\n",
    "categorical_cols_idx = X.describe_categories()[0]\n",
    "X = X.ordinal_encode_categories().fillna(-1)\n",
    "scale_pos_weight = y.shape[0] / y.sum()\n",
    "spotCheckCV(\n",
    "    {\n",
    "        \"XGB\": XGBClassifier(\n",
    "            tree_method=\"hist\",\n",
    "            enable_categorical=True,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "        ),\n",
    "        **with_sampling_strategies(\n",
    "            XGBClassifier(\n",
    "                tree_method=\"hist\",\n",
    "                enable_categorical=True,\n",
    "                scale_pos_weight=scale_pos_weight,\n",
    "            ),\n",
    "            \"XGB\",\n",
    "            categorical_cols_idx,\n",
    "        ),\n",
    "    },\n",
    "    X,\n",
    "    y,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"XGB__max_depth\": np.arange(10, 25, 1),\n",
    "    \"XGB__learning_rate\": [0.01, 0.025, 0.05, 0.085, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    \"XGB__subsample\": np.arange(0.3, 1.0, 0.05),\n",
    "    \"XGB__colsample_bytree\": np.arange(0.1, 1.0, 0.05),\n",
    "    \"XGB__colsample_bylevel\": np.arange(0.6, 1.0, 0.05),\n",
    "    \"XGB__n_estimators\": np.arange(50, 250, 10),\n",
    "    \"XGB__scale_pos_weight\": np.arange(1, 60, 2),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"undersampling\", RandomUnderSampler(sampling_strategy=0.1)),\n",
    "        (\n",
    "            \"XGB\",\n",
    "            XGBClassifier(\n",
    "                tree_method=\"approx\",\n",
    "                enable_categorical=True,\n",
    "                subsample=0.85,\n",
    "                scale_pos_weight=31,\n",
    "                n_estimators=140,\n",
    "                max_depth=13,\n",
    "                learning_rate=0.05,\n",
    "                colsample_bytree=0.7,\n",
    "                colsample_bylevel=0.9,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "evaluate(\n",
    "    model, X_test, y_test, \"XGBoost (non-tuned)\", save=f\"{Notebook.IMAGE_DIR}/eval_xgboost.png\"\n",
    ")\n",
    "\n",
    "Notebook.MODELS[\"XGBoost\"] = model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding\n",
    "\n",
    "We can further minimise false-negatives (at the expense of FPR) by altering the decision threshold manually. Below, we set it such that training recall/sensitivity approaches 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Notebook.MODELS[\"XGBoost\"]\n",
    "\n",
    "threshold = get_threshold(\n",
    "    y_train, \n",
    "    model.predict_proba(X_train)[:, 1], \n",
    ")\n",
    "\n",
    "# Create predictions on the test set, using this new threshold\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = np.where(y_pred_proba > 1-threshold, 1, 0)\n",
    "\n",
    "# Produce scores\n",
    "evaluate_from_pred(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    y_pred_proba,\n",
    "    plot_title=\"XGBoost (tuned)\",\n",
    "    save=f\"{Notebook.IMAGE_DIR}/eval_thresholded_xgboost.png\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = model[\"XGB\"].feature_importances_.argsort()[::-1]\n",
    "fig = sns.barplot(\n",
    "    x=model[\"XGB\"].feature_importances_[sorted_idx],\n",
    "    y=X_train.columns[sorted_idx],\n",
    "    color=\"deepskyblue\",\n",
    ")\n",
    "fig.set_title(\"XGBoost - Global feature importance (gain)\")\n",
    "plt.savefig(f\"{Notebook.IMAGE_DIR}/global_weights_xgboost.png\", bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model[\"XGB\"])\n",
    "shap_values = explainer(X_test)\n",
    "Notebook.EXPLAINERS[\"XGBoost\"] = (explainer, shap_values)\n",
    "\n",
    "fig = shap.plots.beeswarm(shap_values, max_display=30, show=False)\n",
    "plt.title(\"XGBoost\")\n",
    "plt.savefig(f\"{Notebook.IMAGE_DIR}/shap_swarm_xgboost.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_class_idx = np.where(model.predict(X_test))[0][-1]\n",
    "fig = shap.plots.force(shap_values[positive_class_idx])\n",
    "\n",
    "shap.save_html(f\"{Notebook.IMAGE_DIR}/force_plot_xgboost.html\", fig)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Random Forest (balanced)\": RandomForestClassifier(\n",
    "        class_weight=\"balanced_subsample\"\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotCheckDatasets(datasets=datasets, models=models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[\"Mandated vitals, One-hot diagnoses\"]\n",
    "categorical_cols_idx = SCIData(dataset[0]).describe_categories()[0]\n",
    "spotCheckCV(\n",
    "    {\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        **with_sampling_strategies(\n",
    "            RandomForestClassifier(), \"Random Forest\", categorical_cols_idx\n",
    "        ),\n",
    "    },\n",
    "    *dataset\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"undersampling\", RandomUnderSampler(sampling_strategy=0.1)),\n",
    "        (\"randomforest\", RandomForestClassifier()),\n",
    "    ]\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "Notebook.MODELS[\"Random Forest\"] = model\n",
    "\n",
    "evaluate(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    plot_title=\"Random Forest (non-tuned)\",\n",
    "    save=f\"{Notebook.IMAGE_DIR}/eval_random_forest.png\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Notebook.MODELS[\"Random Forest\"]\n",
    "\n",
    "threshold = get_threshold(\n",
    "    y_train, \n",
    "    model.predict_proba(X_train)[:, 1], \n",
    ")\n",
    "\n",
    "# Create predictions on the test set, using this new threshold\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = np.where(y_pred_proba > 1-threshold, 1, 0)\n",
    "\n",
    "# Produce scores\n",
    "evaluate_from_pred(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    y_pred_proba,\n",
    "    plot_title=\"Random Forest (tuned)\",\n",
    "    save=f\"{Notebook.IMAGE_DIR}/eval_thresholded_random_forest.png\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model[\"randomforest\"])\n",
    "shap_values = explainer(X_test)\n",
    "Notebook.EXPLAINERS[\"Random Forest\"] = (explainer, shap_values)\n",
    "\n",
    "fig = shap.plots.beeswarm(shap_values[:, :, 1], max_display=30, show=False)\n",
    "plt.title(\"Random Forest\")\n",
    "plt.savefig(f\"{Notebook.IMAGE_DIR}/shap_swarm_random_forest.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_class_idx = np.where(model.predict(X_test))[0][-1]\n",
    "fig = shap.plots.force(shap_values[positive_class_idx, :, 1])\n",
    "\n",
    "shap.save_html(f\"{Notebook.IMAGE_DIR}/force_plot_random_forest.html\", fig)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest\n",
    "\n",
    "Unlike other models tested, this is a one-class model that detects outliers instead of predicting outcomes. We test the hypothesis that \"abnormal\" values associated with the tracked outcome will be outliers compared to the normal ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Isolation Forest\": IsolationForestWrapper(),\n",
    "    \"Isolation Forest (contamination)\": IsolationForestWrapper(),\n",
    "}\n",
    "isolation_forest_set_params = {\n",
    "    \"Isolation Forest (contamination)\": lambda X, y: dict(\n",
    "        contamination=(y.sum()) / y.shape[0]\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotCheckDatasets(\n",
    "    datasets=datasets, models=models, set_params=isolation_forest_set_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Mandated vitals, Categorical diagnoses (main only)\n",
    "(X_train_if, y_train_if), (X_test_if, y_test_if) = (\n",
    "    sci_train.drop(SCICols.diagnoses[1:], axis=1).xy(\n",
    "        outcome=Notebook.OUTCOME, ordinal_encoding=True, fillna=True\n",
    "    ),\n",
    "    sci_test.drop(SCICols.diagnoses[1:], axis=1).xy(\n",
    "        outcome=Notebook.OUTCOME, ordinal_encoding=True, fillna=True\n",
    "    ),\n",
    ")\n",
    "drop_exclusive_cols(X_train_if, X_test_if)\n",
    "\n",
    "# Train without positive labelled instances\n",
    "model = IsolationForestWrapper().fit(X_train_if[~y_train_if].to_numpy())\n",
    "\n",
    "Notebook.MODELS[\"Isolation Forest\"] = model\n",
    "\n",
    "evaluate(\n",
    "    model,\n",
    "    X_test_if.to_numpy(),\n",
    "    y_test_if,\n",
    "    plot_title=\"Isolation Forest (non-tuned)\",\n",
    "    save=f\"{Notebook.IMAGE_DIR}/eval_isolation_forest.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Notebook.MODELS[\"Isolation Forest\"]\n",
    "\n",
    "y_pred_proba_train = model.decision_function(X_train_if)\n",
    "y_pred_proba_train -= y_pred_proba_train.min()\n",
    "\n",
    "threshold = get_threshold(\n",
    "    y_train_if, \n",
    "    y_pred_proba_train, \n",
    ")\n",
    "\n",
    "# Create predictions on the test set, using this new threshold\n",
    "y_pred_proba = model.decision_function(X_test_if)\n",
    "y_pred = np.where(y_pred_proba-y_pred_proba.min() > threshold, 1, 0)\n",
    "\n",
    "evaluate_from_pred(\n",
    "    y_test_if,\n",
    "    y_pred,\n",
    "    y_pred_proba,\n",
    "    plot_title=\"Isolation Forest (tuned)\",\n",
    "    save=f\"{Notebook.IMAGE_DIR}/eval_thresholded_isolation_forest.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(X_test_if)\n",
    "shap_values.values = -shap_values.values  # Fix for isolation forest's unique labelling\n",
    "Notebook.EXPLAINERS[\"Isolation Forest\"] = (explainer, shap_values)\n",
    "\n",
    "fig = shap.plots.beeswarm(shap_values, max_display=30, show=False)\n",
    "plt.title(\"Isolation Forest\")\n",
    "plt.savefig(f\"{Notebook.IMAGE_DIR}/shap_swarm_isolation_forest.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_class_idx = np.where(model.predict(X_test_if.to_numpy()))[0][-1]\n",
    "fig = shap.plots.force(shap_values[positive_class_idx])\n",
    "\n",
    "shap.save_html(f\"{Notebook.IMAGE_DIR}/force_plot_random_forest.html\", fig)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lightgbm_set_params = {\n",
    "    \"LightGBM Weighted\": lambda X, y: dict(scale_pos_weight=y.shape[0] / y.sum())\n",
    "}\n",
    "\n",
    "spotCheckDatasets(\n",
    "    datasets=datasets_xgb,\n",
    "    models={\n",
    "        \"LightGBM\": LGBMClassifier(metric=[\"l2\", \"auc\"]),\n",
    "        \"LightGBM Balanced\": LGBMClassifier(metric=[\"l2\", \"auc\"], is_unbalance=True),\n",
    "        \"LightGBM Weighted\": LGBMClassifier(metric=[\"l2\", \"auc\"]),\n",
    "    },\n",
    "    set_params=lightgbm_set_params,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets_xgb[\"Mandated vitals, One-hot diagnoses\"]\n",
    "categorical_cols_idx = X.describe_categories()[0]\n",
    "X = X.ordinal_encode_categories()\n",
    "result = spotCheckCV(\n",
    "    {\n",
    "        \"LightGBM\": LGBMClassifier(\n",
    "            metric=[\"l2\", \"auc\"], scale_pos_weight=y.shape[0] / y.sum()\n",
    "        ),\n",
    "        **with_sampling_strategies(\n",
    "            LGBMClassifier(metric=[\"l2\", \"auc\"], scale_pos_weight=y.shape[0] / y.sum()),\n",
    "            \"LightGBM\",\n",
    "            categorical_cols_idx,\n",
    "        ),\n",
    "    },\n",
    "    X,\n",
    "    y,\n",
    ")\n",
    "\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.085, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    \"boosting_type\": [\"gbdt\", \"dart\", \"goss\"],\n",
    "    \"sub_feature\": np.arange(0, 1, 0.05),\n",
    "    \"num_leaves\": np.arange(20, 300, 20),\n",
    "    \"min_data\": np.arange(10, 100, 10),\n",
    "    \"max_depth\": np.arange(5, 200, 20),\n",
    "    \"scale_pos_weight\": np.arange(1, 60, 2),\n",
    "    \"colsample_bytree\": np.arange(0.1, 1.0, 0.05),\n",
    "    \"subsample\": np.arange(0.3, 1.0, 0.05),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train_lgbm, y_train_lgbm), (X_test_lgbm, y_test_lgbm) = \\\n",
    "    sci_train.encode_ccs_onehot().xy(outcome=Notebook.OUTCOME, fillna=True), \\\n",
    "    sci_test.encode_ccs_onehot().xy(outcome=Notebook.OUTCOME, fillna=True),\n",
    "\n",
    "drop_exclusive_cols(X_train_lgbm, X_test_lgbm)\n",
    "\n",
    "model = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"undersampling\", RandomUnderSampler(sampling_strategy=0.1)),\n",
    "        (\"lightgbm\", LGBMClassifier(metric=[\"l2\", \"auc\"], is_unbalance=True)),\n",
    "    ]\n",
    ").fit(X_train_lgbm.copy(), y_train_lgbm)\n",
    "\n",
    "Notebook.MODELS[\"LightGBM\"] = model\n",
    "\n",
    "evaluate(\n",
    "    model,\n",
    "    X_test_lgbm.copy(),\n",
    "    y_test_lgbm,\n",
    "    plot_title=\"LightGBM (non-tuned)\",\n",
    "    save=f\"{Notebook.IMAGE_DIR}/eval_lightgbm.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding\n",
    "We adjust the classification threshold to further minimise false-negatives. Here we set it such that training recall/sensitivity is 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Notebook.MODELS[\"LightGBM\"]\n",
    "\n",
    "threshold = get_threshold(\n",
    "    y_test_lgbm, \n",
    "    model.predict_proba(X_test_lgbm)[:, 1], \n",
    ")\n",
    "\n",
    "# Create predictions on the test set, using this new threshold\n",
    "y_pred_proba = model.predict_proba(X_test_lgbm)[:, 1]\n",
    "y_pred = np.where(y_pred_proba > threshold, 1, 0)\n",
    "\n",
    "evaluate_from_pred(\n",
    "    y_test_lgbm,\n",
    "    y_pred,\n",
    "    y_pred_proba,\n",
    "    plot_title=\"LightGBM (tuned)\",\n",
    "    save=f\"{Notebook.IMAGE_DIR}/eval_thresholded_lightgbm.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "fig = lgb.plot_importance(model[\"lightgbm\"])\n",
    "fig.set_title(\"LightGBM - Global feature importance (gain)\")\n",
    "plt.savefig(f\"{Notebook.IMAGE_DIR}/global_weights_lightgbm.png\", bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model[\"lightgbm\"])\n",
    "shap_values = explainer(X_test)\n",
    "Notebook.EXPLAINERS[\"LightGBM\"] = (explainer, shap_values)\n",
    "\n",
    "fig = shap.plots.beeswarm(shap_values[:, :, 1], max_display=30, show=False)\n",
    "plt.title(\"LightGBM\")\n",
    "plt.savefig(f\"{Notebook.IMAGE_DIR}/shap_swarm_lightgbm.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_class_idx = np.where(model.predict(X_test_lgbm))[0][-1]\n",
    "fig = shap.plots.force(shap_values[positive_class_idx, :, 1])\n",
    "\n",
    "shap.save_html(f\"{Notebook.IMAGE_DIR}/force_plot_lightgbm.html\", fig)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet\n",
    "\n",
    "Deep-learning model for outcome prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_idx, categorical_cols_dims = X_train.describe_categories()\n",
    "\n",
    "(X_train_tn, X_valid_tn, y_train_tn, y_valid_tn) = train_test_split(\n",
    "    X_train.to_numpy(),\n",
    "    y_train.to_numpy(),\n",
    "    stratify=y_train,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "tabnet_params = dict(\n",
    "    n_a=24,\n",
    "    n_d=24,\n",
    "    cat_idxs=categorical_cols_idx,\n",
    "    cat_dims=categorical_cols_dims,\n",
    "    cat_emb_dim=1,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=0.1),\n",
    "    scheduler_params=dict(step_size=50, gamma=0.7),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type=\"entmax\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "model = TabNetClassifier(**tabnet_params)\n",
    "\n",
    "model.fit(\n",
    "    X_train=X_train_tn,\n",
    "    y_train=y_train_tn,\n",
    "    eval_set=[(X_train_tn, y_train_tn), (X_valid_tn, y_valid_tn),],\n",
    "    eval_name=[\"train\", \"valid\"],\n",
    "    eval_metric=[F2TabNet],\n",
    "    max_epochs=300,\n",
    "    patience=50,\n",
    "    batch_size=512,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "Notebook.MODELS[\"TabNet\"] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    model, X_test.to_numpy(), y_test, \"TabNet (non-tuned)\", save=f\"{Notebook.IMAGE_DIR}/eval_tabnet.png\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Notebook.MODELS[\"TabNet\"]\n",
    "\n",
    "threshold = get_threshold(\n",
    "    y_test, \n",
    "    model.predict_proba(X_test.to_numpy())[:, 1],\n",
    "    target=0.785\n",
    ")\n",
    "\n",
    "# Create predictions on the test set, using this new threshold\n",
    "y_pred_proba = model.predict_proba(X_test.to_numpy())[:, 1]\n",
    "y_pred = np.where(y_pred_proba > threshold, 1, 0)\n",
    "\n",
    "evaluate_from_pred(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    y_pred_proba,\n",
    "    plot_title=\"TabNet (tuned)\",\n",
    "    save=f\"{Notebook.IMAGE_DIR}/eval_thresholded_tabnet.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = model.feature_importances_.argsort()[::-1]\n",
    "fig = sns.barplot(\n",
    "    x=model.feature_importances_[sorted_idx],\n",
    "    y=X_train.columns[sorted_idx],\n",
    "    color=\"deepskyblue\",\n",
    ")\n",
    "fig.set_title(\"TabNet - Global feature importance\")\n",
    "plt.savefig(f\"{Notebook.IMAGE_DIR}/global_weights_tabnet.png\", bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_class_idx = np.where(model.predict(X_test.to_numpy()))[0][-1]\n",
    "explain_matrix, masks = model.explain(\n",
    "    X_test.to_numpy()[positive_class_idx : positive_class_idx + 1]\n",
    ")\n",
    "list(sorted(zip(X_test.columns, explain_matrix[0]), key=lambda x: x[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = pd.DataFrame({\n",
    "    key: model.predict({'LightGBM': X_test_lgbm,'Isolation Forest': X_test_if,'TabNet': X_test.to_numpy(),'XGBoost': X_test,'Random Forest': X_test,'Logistic Regression': X_test}[key])\n",
    "    for key, model in Notebook.MODELS.items()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = y_preds[y_preds.all(axis=1)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = {\n",
    "    key: shap_value for key, (explainer, shap_value) in Notebook.EXPLAINERS.items()\n",
    "}\n",
    "shap_values['LightGBM'] = shap_values['LightGBM'][:,:,1]\n",
    "shap_values['Random Forest'] = shap_values['Random Forest'][:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelkey, shap_value in shap_values.items():\n",
    "    fig = shap.plots.force(shap_value[candidates[0]])\n",
    "    shap.save_html(f\"{Notebook.IMAGE_DIR}/comaprison_force_plot_{modelkey.replace(' ','')}.html\", fig)\n",
    "    \n",
    "    print(modelkey)\n",
    "    display(fig)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e35166262197b1ea9223463adaf11f6b58d81a82b7650a41ad4f3574b9c5682"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
